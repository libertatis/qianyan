{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# DuReader Robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting paddlenlp\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/62/10/ccc761d3e3a994703f31a4d0f93db0d13789d1c624a0cbbe9fe6439ed601/paddlenlp-2.0.5-py3-none-any.whl (435kB)\n",
      "\u001b[K     |████████████████████████████████| 440kB 10.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.2.3)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.5)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (2.4.2)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->visualdl->paddlenlp) (56.2.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (7.2.0)\n",
      "Installing collected packages: paddlenlp\n",
      "  Found existing installation: paddlenlp 2.0.1\n",
      "    Uninstalling paddlenlp-2.0.1:\n",
      "      Successfully uninstalled paddlenlp-2.0.1\n",
      "Successfully installed paddlenlp-2.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade paddlenlp -i https://mirror.baidu.com/pypi/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\r\n",
    "import math\r\n",
    "import os\r\n",
    "import random\r\n",
    "import time\r\n",
    "from functools import partial\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "import paddlenlp as ppnlp\r\n",
    "from paddle.io import DataLoader\r\n",
    "from paddle.io import BatchSampler\r\n",
    "from paddle.io import DistributedBatchSampler\r\n",
    "from paddlenlp.data import Dict\r\n",
    "from paddlenlp.data import Pad\r\n",
    "from paddlenlp.data import Stack\r\n",
    "from paddlenlp.data import Tuple\r\n",
    "from paddlenlp.datasets import load_dataset\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "from paddlenlp.ops.optimizer import AdamW\r\n",
    "from paddlenlp.transformers import BertForQuestionAnswering\r\n",
    "from paddlenlp.transformers import BertTokenizer\r\n",
    "from paddlenlp.transformers import ErnieForQuestionAnswering\r\n",
    "from paddlenlp.transformers import ErnieTokenizer\r\n",
    "from paddlenlp.transformers import ErnieGramForQuestionAnswering\r\n",
    "from paddlenlp.transformers import ErnieGramModel\r\n",
    "from paddlenlp.transformers import ErnieGramTokenizer\r\n",
    "from paddlenlp.transformers import RobertaForQuestionAnswering\r\n",
    "from paddlenlp.transformers import RobertaTokenizer\r\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\r\n",
    "\r\n",
    "from config import Config\r\n",
    "from utils import CrossEntropyLossForSQuAD\r\n",
    "from utils import evaluate\r\n",
    "from utils import predict\r\n",
    "from utils import prepare_train_features\r\n",
    "from utils import prepare_validation_features\r\n",
    "from utils import set_seed\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\r\n",
    "    \"bert\": (BertForQuestionAnswering, BertTokenizer),\r\n",
    "    \"ernie\": (ErnieForQuestionAnswering, ErnieTokenizer),\r\n",
    "    \"ernie_gram\": (ErnieGramForQuestionAnswering, ErnieGramTokenizer),\r\n",
    "    \"roberta\": (RobertaForQuestionAnswering, RobertaTokenizer)\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_train(args):\r\n",
    "    \r\n",
    "    paddle.set_device(args.device)\r\n",
    "\r\n",
    "    args.model_type = args.model_type.lower()\r\n",
    "    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\r\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\r\n",
    "\r\n",
    "    set_seed(args)\r\n",
    "\r\n",
    "    # train_ds = load_dataset('cmrc2018', splits=['train'])\r\n",
    "    # dev_ds = load_dataset('dureader_robust', splits=['dev'])\r\n",
    "    train_robust, dev_ds = load_dataset('dureader_robust', splits=['train', 'dev'])\r\n",
    "    train_cmrc, dev_cmrc = load_dataset('cmrc2018', splits=['train', 'dev'])\r\n",
    "\r\n",
    "    train_dataset = []\r\n",
    "    for idx, example in enumerate(train_robust):\r\n",
    "        train_dataset.append(example)\r\n",
    "    for idx, example in enumerate(train_cmrc):\r\n",
    "        train_dataset.append(example)\r\n",
    "    for idx, example in enumerate(dev_cmrc):\r\n",
    "        train_dataset.append(example)\r\n",
    "    train_ds = MapDataset(train_dataset)\r\n",
    "\r\n",
    "    train_trans_func = partial(\r\n",
    "        prepare_train_features, \r\n",
    "        max_seq_length=args.max_seq_length, \r\n",
    "        doc_stride=args.doc_stride,\r\n",
    "        tokenizer=tokenizer\r\n",
    "    )\r\n",
    "\r\n",
    "    train_ds.map(train_trans_func, batched=True)\r\n",
    "\r\n",
    "    dev_trans_func = partial(\r\n",
    "        prepare_validation_features, \r\n",
    "        max_seq_length=args.max_seq_length, \r\n",
    "        doc_stride=args.doc_stride,\r\n",
    "        tokenizer=tokenizer\r\n",
    "    )\r\n",
    "\r\n",
    "    dev_ds.map(dev_trans_func, batched=True)\r\n",
    "\r\n",
    "    # 定义BatchSampler\r\n",
    "    train_batch_sampler = DistributedBatchSampler(\r\n",
    "            dataset=train_ds, \r\n",
    "            batch_size=args.batch_size, \r\n",
    "            shuffle=True\r\n",
    "    )\r\n",
    "    dev_batch_sampler = BatchSampler(\r\n",
    "        dataset=dev_ds, \r\n",
    "        batch_size=args.batch_size, \r\n",
    "        shuffle=False\r\n",
    "    )\r\n",
    "    # 定义batchify_fn\r\n",
    "    train_batchify_fn = lambda samples, fn=Dict({\r\n",
    "        \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "        \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\r\n",
    "        \"start_positions\": Stack(dtype=\"int64\"),\r\n",
    "        \"end_positions\": Stack(dtype=\"int64\")\r\n",
    "    }): fn(samples)\r\n",
    "\r\n",
    "    dev_batchify_fn = lambda samples, fn=Dict({\r\n",
    "        \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "        \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id)\r\n",
    "    }): fn(samples)\r\n",
    "\r\n",
    "    # 构造DataLoader\r\n",
    "    train_data_loader = DataLoader(\r\n",
    "        dataset=train_ds,\r\n",
    "        batch_sampler=train_batch_sampler,\r\n",
    "        collate_fn=train_batchify_fn,\r\n",
    "        return_list=True\r\n",
    "    )\r\n",
    "\r\n",
    "    dev_data_loader =  DataLoader(\r\n",
    "        dataset=dev_ds,\r\n",
    "        batch_sampler=dev_batch_sampler,\r\n",
    "        collate_fn=dev_batchify_fn,\r\n",
    "        return_list=True\r\n",
    "    )\r\n",
    "\r\n",
    "    output_dir = os.path.join(args.output_dir, 'best_model')\r\n",
    "    if not os.path.exists(output_dir):\r\n",
    "        os.makedirs(output_dir)\r\n",
    "\r\n",
    "    init_checkpoint = os.path.join(args.output_dir, 'model_cmrc2018')\r\n",
    "    if not os.path.exists(init_checkpoint):\r\n",
    "        os.makedirs(init_checkpoint)\r\n",
    "\r\n",
    "    model = model_class.from_pretrained(args.model_name_or_path)\r\n",
    "    # model = model_class.from_pretrained(init_checkpoint)\r\n",
    "    # model = model_class.from_pretrained(output_dir)\r\n",
    "\r\n",
    "\r\n",
    "    num_training_steps = args.max_steps if args.max_steps > 0 else len(\r\n",
    "        train_data_loader) * args.num_train_epochs\r\n",
    "    num_train_epochs = math.ceil(num_training_steps / len(train_data_loader))\r\n",
    "\r\n",
    "    num_batches = len(train_data_loader)\r\n",
    "\r\n",
    "    lr_scheduler = LinearDecayWithWarmup(\r\n",
    "        learning_rate=args.learning_rate, \r\n",
    "        total_steps=num_training_steps,\r\n",
    "        warmup=args.warmup_proportion\r\n",
    "    )\r\n",
    "\r\n",
    "    # lr_scheduler = paddle.optimizer.lr.LinearWarmup(\r\n",
    "    #     learning_rate=args.learning_rate, \r\n",
    "    #     warmup_steps=400, \r\n",
    "    #     start_lr=0, \r\n",
    "    #     end_lr=args.learning_rate, \r\n",
    "    #     verbose=False\r\n",
    "    # )\r\n",
    "\r\n",
    "    # Generate parameter names needed to perform weight decay.\r\n",
    "    # All bias and LayerNorm parameters are excluded.\r\n",
    "    decay_params = [\r\n",
    "        p.name for n, p in model.roberta.named_parameters()\r\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "    ]\r\n",
    "    optimizer = paddle.optimizer.AdamW(\r\n",
    "        learning_rate=lr_scheduler,\r\n",
    "        epsilon=args.adam_epsilon,\r\n",
    "        parameters=model.parameters(),\r\n",
    "        weight_decay=args.weight_decay,\r\n",
    "        apply_decay_param_fun=lambda x: x in decay_params\r\n",
    "    )\r\n",
    "\r\n",
    "    criterion = CrossEntropyLossForSQuAD()\r\n",
    "\r\n",
    "    best_val_f1 = 0.0\r\n",
    "    accumulation_steps = 8\r\n",
    "\r\n",
    "    global_step = 0\r\n",
    "    tic_train = time.time()\r\n",
    "    for epoch in range(1, num_train_epochs + 1):\r\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\r\n",
    "\r\n",
    "            global_step += 1\r\n",
    "            \r\n",
    "            input_ids, segment_ids, start_positions, end_positions = batch\r\n",
    "            logits = model(input_ids=input_ids, token_type_ids=segment_ids)\r\n",
    "            loss = criterion(logits, (start_positions, end_positions))\r\n",
    "\r\n",
    "            if global_step % args.log_steps == 0 :\r\n",
    "                # print(\"global step %d, epoch: %d, batch: %d/%d, loss: %.5f,  speed: %.2f step/s\" % (\r\n",
    "                #     global_step, epoch, step, num_batches, loss, args.log_steps / (time.time() - tic_train)))\r\n",
    "                \r\n",
    "                print(\"global step %d, epoch: %d, batch: %d/%d, loss: %.5f,  speed: %.2f step/s, lr: %1.16e\" % (\r\n",
    "                    global_step, epoch, step, num_batches, loss, args.log_steps / (time.time() - tic_train), lr_scheduler.get_lr()))\r\n",
    "                \r\n",
    "                tic_train = time.time()\r\n",
    "            \r\n",
    "            # loss = loss / accumulation_steps\r\n",
    "            loss.backward()\r\n",
    "\r\n",
    "            # if global_step % accumulation_steps == 0:\r\n",
    "            # if step % accumulation_steps == 0:\r\n",
    "            optimizer.step()\r\n",
    "            lr_scheduler.step()\r\n",
    "            optimizer.clear_grad()\r\n",
    "\r\n",
    "            if global_step % args.save_steps == 0 or global_step == num_training_steps:\r\n",
    "                em, f1 = evaluate(model=model, data_loader=dev_data_loader)\r\n",
    "\r\n",
    "                print(\"global step: %d, eval dev Exact Mactch: %.5f, f1_score: %.5f\" % (global_step, em, f1))\r\n",
    "\r\n",
    "                if f1 > best_val_f1:\r\n",
    "                    best_val_f1 = f1\r\n",
    "\r\n",
    "                    print(\"save model at global step: %d, best eval f1_score: %.5f\" % (global_step, best_val_f1))\r\n",
    "\r\n",
    "                    model.save_pretrained(output_dir)\r\n",
    "                    tokenizer.save_pretrained(output_dir)\r\n",
    "\r\n",
    "                if global_step == num_training_steps:\r\n",
    "                    break\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_predict(args):\r\n",
    "\r\n",
    "    paddle.set_device(args.device)\r\n",
    "\r\n",
    "    output_dir = os.path.join(args.output_dir, \"best_model\")\r\n",
    "\r\n",
    "    # 1. 加载测试集\r\n",
    "    test_ds = load_dataset('dureader_robust', splits='test')\r\n",
    "\r\n",
    "    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\r\n",
    "    tokenizer = tokenizer_class.from_pretrained(output_dir)\r\n",
    "\r\n",
    "    # 2. 转化为 id\r\n",
    "    test_trans_func = partial(\r\n",
    "        prepare_validation_features, \r\n",
    "        max_seq_length=args.max_seq_length, \r\n",
    "        doc_stride=args.doc_stride,\r\n",
    "        tokenizer=tokenizer\r\n",
    "    )\r\n",
    "    test_ds.map(test_trans_func, batched=True)\r\n",
    "\r\n",
    "    # test BatchSampler\r\n",
    "    test_batch_sampler = BatchSampler(\r\n",
    "        dataset=test_ds, \r\n",
    "        batch_size=args.batch_size, \r\n",
    "        shuffle=False\r\n",
    "    )\r\n",
    "\r\n",
    "    # test dataset features batchify\r\n",
    "    test_batchify_fn = lambda samples, fn=Dict({\r\n",
    "        \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "        \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id)\r\n",
    "    }): fn(samples)\r\n",
    "\r\n",
    "    # test DataLoader\r\n",
    "    test_data_loader =  DataLoader(\r\n",
    "        dataset=test_ds,\r\n",
    "        batch_sampler=test_batch_sampler,\r\n",
    "        collate_fn=test_batchify_fn,\r\n",
    "        return_list=True\r\n",
    "    )\r\n",
    "\r\n",
    "    model = model_class.from_pretrained(output_dir)\r\n",
    "    \r\n",
    "    all_predictions = predict(model, test_data_loader)\r\n",
    "\r\n",
    "    # Can also write all_nbest_json and scores_diff_json files if needed\r\n",
    "    with open('prediction.json', \"w\", encoding='utf-8') as writer:\r\n",
    "        writer.write(\r\n",
    "            json.dumps(\r\n",
    "                all_predictions, ensure_ascii=False, indent=4) + \"\\n\")\r\n",
    "\r\n",
    "    count = 0\r\n",
    "    for example in test_data_loader.dataset.data:\r\n",
    "        count += 1\r\n",
    "        print()\r\n",
    "        print('问题：',example['question'])\r\n",
    "        print('原文：',''.join(example['context']))\r\n",
    "        print('答案：',all_predictions[example['id']])\r\n",
    "        if count >= 5:\r\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = Config(model_type='roberta', \r\n",
    "              model_name_or_path='roberta-wwm-ext-large', \r\n",
    "              output_dir='./outputs/dureader-robust/',\r\n",
    "              \r\n",
    "              max_seq_length=384,\r\n",
    "              batch_size=22, \r\n",
    "              learning_rate=5e-5,\r\n",
    "              num_train_epochs=10,\r\n",
    "              log_steps=10,\r\n",
    "              save_steps=500,\r\n",
    "              warmup_proportion=0.1,\r\n",
    "              weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-07-06 23:05:33,999] [    INFO] - Found /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/vocab.txt\n",
      "[2021-07-06 23:07:57,384] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/roberta_chn_large.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10/2350, loss: 5.86297,  speed: 0.71 step/s, lr: 1.9148936170212765e-07\n",
      "global step 20, epoch: 1, batch: 20/2350, loss: 5.86746,  speed: 0.67 step/s, lr: 4.0425531914893619e-07\n",
      "global step 30, epoch: 1, batch: 30/2350, loss: 5.92419,  speed: 0.67 step/s, lr: 6.1702127659574471e-07\n",
      "global step 40, epoch: 1, batch: 40/2350, loss: 5.76049,  speed: 0.67 step/s, lr: 8.2978723404255318e-07\n",
      "global step 50, epoch: 1, batch: 50/2350, loss: 5.56990,  speed: 0.67 step/s, lr: 1.0425531914893618e-06\n",
      "global step 60, epoch: 1, batch: 60/2350, loss: 5.51904,  speed: 0.66 step/s, lr: 1.2553191489361703e-06\n",
      "global step 70, epoch: 1, batch: 70/2350, loss: 5.41459,  speed: 0.66 step/s, lr: 1.4680851063829787e-06\n",
      "global step 80, epoch: 1, batch: 80/2350, loss: 5.29762,  speed: 0.66 step/s, lr: 1.6808510638297873e-06\n",
      "global step 90, epoch: 1, batch: 90/2350, loss: 4.90931,  speed: 0.66 step/s, lr: 1.8936170212765956e-06\n",
      "global step 100, epoch: 1, batch: 100/2350, loss: 4.69507,  speed: 0.66 step/s, lr: 2.1063829787234044e-06\n",
      "global step 110, epoch: 1, batch: 110/2350, loss: 4.70151,  speed: 0.66 step/s, lr: 2.3191489361702130e-06\n",
      "global step 120, epoch: 1, batch: 120/2350, loss: 4.30305,  speed: 0.66 step/s, lr: 2.5319148936170216e-06\n",
      "global step 130, epoch: 1, batch: 130/2350, loss: 4.66603,  speed: 0.66 step/s, lr: 2.7446808510638301e-06\n",
      "global step 140, epoch: 1, batch: 140/2350, loss: 3.93041,  speed: 0.66 step/s, lr: 2.9574468085106383e-06\n",
      "global step 150, epoch: 1, batch: 150/2350, loss: 4.45234,  speed: 0.66 step/s, lr: 3.1702127659574468e-06\n",
      "global step 160, epoch: 1, batch: 160/2350, loss: 3.93340,  speed: 0.66 step/s, lr: 3.3829787234042558e-06\n",
      "global step 170, epoch: 1, batch: 170/2350, loss: 3.67015,  speed: 0.66 step/s, lr: 3.5957446808510640e-06\n",
      "global step 180, epoch: 1, batch: 180/2350, loss: 3.64141,  speed: 0.66 step/s, lr: 3.8085106382978726e-06\n",
      "global step 190, epoch: 1, batch: 190/2350, loss: 3.82291,  speed: 0.66 step/s, lr: 4.0212765957446807e-06\n",
      "global step 200, epoch: 1, batch: 200/2350, loss: 2.96495,  speed: 0.66 step/s, lr: 4.2340425531914897e-06\n",
      "global step 210, epoch: 1, batch: 210/2350, loss: 2.64955,  speed: 0.66 step/s, lr: 4.4468085106382979e-06\n",
      "global step 220, epoch: 1, batch: 220/2350, loss: 2.30736,  speed: 0.66 step/s, lr: 4.6595744680851069e-06\n",
      "global step 230, epoch: 1, batch: 230/2350, loss: 1.98962,  speed: 0.65 step/s, lr: 4.8723404255319150e-06\n",
      "global step 240, epoch: 1, batch: 240/2350, loss: 1.65873,  speed: 0.66 step/s, lr: 5.0851063829787240e-06\n",
      "global step 250, epoch: 1, batch: 250/2350, loss: 2.25084,  speed: 0.65 step/s, lr: 5.2978723404255322e-06\n",
      "global step 260, epoch: 1, batch: 260/2350, loss: 2.09735,  speed: 0.66 step/s, lr: 5.5106382978723403e-06\n",
      "global step 270, epoch: 1, batch: 270/2350, loss: 2.23961,  speed: 0.66 step/s, lr: 5.7234042553191493e-06\n",
      "global step 280, epoch: 1, batch: 280/2350, loss: 2.50628,  speed: 0.66 step/s, lr: 5.9361702127659575e-06\n",
      "global step 290, epoch: 1, batch: 290/2350, loss: 1.68214,  speed: 0.66 step/s, lr: 6.1489361702127664e-06\n",
      "global step 300, epoch: 1, batch: 300/2350, loss: 1.83221,  speed: 0.66 step/s, lr: 6.3617021276595746e-06\n",
      "global step 310, epoch: 1, batch: 310/2350, loss: 1.73232,  speed: 0.65 step/s, lr: 6.5744680851063836e-06\n",
      "global step 320, epoch: 1, batch: 320/2350, loss: 1.74113,  speed: 0.66 step/s, lr: 6.7872340425531926e-06\n",
      "global step 330, epoch: 1, batch: 330/2350, loss: 1.15104,  speed: 0.66 step/s, lr: 7.0000000000000007e-06\n",
      "global step 340, epoch: 1, batch: 340/2350, loss: 1.30536,  speed: 0.65 step/s, lr: 7.2127659574468080e-06\n",
      "global step 350, epoch: 1, batch: 350/2350, loss: 1.49552,  speed: 0.66 step/s, lr: 7.4255319148936170e-06\n",
      "global step 360, epoch: 1, batch: 360/2350, loss: 1.29716,  speed: 0.66 step/s, lr: 7.6382978723404260e-06\n",
      "global step 370, epoch: 1, batch: 370/2350, loss: 1.59626,  speed: 0.66 step/s, lr: 7.8510638297872350e-06\n",
      "global step 380, epoch: 1, batch: 380/2350, loss: 1.37617,  speed: 0.66 step/s, lr: 8.0638297872340423e-06\n",
      "global step 390, epoch: 1, batch: 390/2350, loss: 1.48473,  speed: 0.65 step/s, lr: 8.2765957446808513e-06\n",
      "global step 400, epoch: 1, batch: 400/2350, loss: 1.44410,  speed: 0.66 step/s, lr: 8.4893617021276603e-06\n",
      "global step 410, epoch: 1, batch: 410/2350, loss: 1.38901,  speed: 0.66 step/s, lr: 8.7021276595744693e-06\n",
      "global step 420, epoch: 1, batch: 420/2350, loss: 1.41804,  speed: 0.66 step/s, lr: 8.9148936170212783e-06\n",
      "global step 430, epoch: 1, batch: 430/2350, loss: 1.07452,  speed: 0.66 step/s, lr: 9.1276595744680856e-06\n",
      "global step 440, epoch: 1, batch: 440/2350, loss: 1.21127,  speed: 0.66 step/s, lr: 9.3404255319148946e-06\n",
      "global step 450, epoch: 1, batch: 450/2350, loss: 0.86581,  speed: 0.66 step/s, lr: 9.5531914893617036e-06\n",
      "global step 460, epoch: 1, batch: 460/2350, loss: 1.49034,  speed: 0.66 step/s, lr: 9.7659574468085109e-06\n",
      "global step 470, epoch: 1, batch: 470/2350, loss: 1.12867,  speed: 0.66 step/s, lr: 9.9787234042553182e-06\n",
      "global step 480, epoch: 1, batch: 480/2350, loss: 1.64415,  speed: 0.66 step/s, lr: 1.0191489361702127e-05\n",
      "global step 490, epoch: 1, batch: 490/2350, loss: 1.76722,  speed: 0.66 step/s, lr: 1.0404255319148936e-05\n",
      "global step 500, epoch: 1, batch: 500/2350, loss: 1.45616,  speed: 0.66 step/s, lr: 1.0617021276595745e-05\n",
      "Processing example: 1000\n",
      "time per 1000: 24.19329857826233\n",
      "global step: 500, eval dev Exact Mactch: 0.69866, f1_score: 0.85219\n",
      "save model at global step: 500, best eval f1_score: 0.85219\n",
      "global step 510, epoch: 1, batch: 510/2350, loss: 1.84487,  speed: 0.12 step/s, lr: 1.0829787234042554e-05\n",
      "global step 520, epoch: 1, batch: 520/2350, loss: 1.39570,  speed: 0.66 step/s, lr: 1.1042553191489362e-05\n",
      "global step 530, epoch: 1, batch: 530/2350, loss: 1.17478,  speed: 0.66 step/s, lr: 1.1255319148936171e-05\n",
      "global step 540, epoch: 1, batch: 540/2350, loss: 1.14649,  speed: 0.66 step/s, lr: 1.1468085106382980e-05\n",
      "global step 550, epoch: 1, batch: 550/2350, loss: 0.93878,  speed: 0.66 step/s, lr: 1.1680851063829789e-05\n",
      "global step 560, epoch: 1, batch: 560/2350, loss: 1.82680,  speed: 0.66 step/s, lr: 1.1893617021276596e-05\n",
      "global step 570, epoch: 1, batch: 570/2350, loss: 1.32213,  speed: 0.66 step/s, lr: 1.2106382978723405e-05\n",
      "global step 580, epoch: 1, batch: 580/2350, loss: 1.05089,  speed: 0.66 step/s, lr: 1.2319148936170214e-05\n",
      "global step 590, epoch: 1, batch: 590/2350, loss: 1.14012,  speed: 0.66 step/s, lr: 1.2531914893617023e-05\n",
      "global step 600, epoch: 1, batch: 600/2350, loss: 1.29881,  speed: 0.66 step/s, lr: 1.2744680851063832e-05\n",
      "global step 610, epoch: 1, batch: 610/2350, loss: 1.44007,  speed: 0.66 step/s, lr: 1.2957446808510639e-05\n",
      "global step 620, epoch: 1, batch: 620/2350, loss: 1.31606,  speed: 0.66 step/s, lr: 1.3170212765957448e-05\n",
      "global step 630, epoch: 1, batch: 630/2350, loss: 1.57839,  speed: 0.66 step/s, lr: 1.3382978723404255e-05\n",
      "global step 640, epoch: 1, batch: 640/2350, loss: 1.18928,  speed: 0.66 step/s, lr: 1.3595744680851064e-05\n",
      "global step 650, epoch: 1, batch: 650/2350, loss: 1.33758,  speed: 0.66 step/s, lr: 1.3808510638297872e-05\n",
      "global step 660, epoch: 1, batch: 660/2350, loss: 1.31162,  speed: 0.66 step/s, lr: 1.4021276595744681e-05\n",
      "global step 670, epoch: 1, batch: 670/2350, loss: 1.40138,  speed: 0.66 step/s, lr: 1.4234042553191490e-05\n",
      "global step 680, epoch: 1, batch: 680/2350, loss: 1.85670,  speed: 0.66 step/s, lr: 1.4446808510638299e-05\n",
      "global step 690, epoch: 1, batch: 690/2350, loss: 1.51368,  speed: 0.66 step/s, lr: 1.4659574468085108e-05\n",
      "global step 700, epoch: 1, batch: 700/2350, loss: 1.41649,  speed: 0.66 step/s, lr: 1.4872340425531917e-05\n",
      "global step 710, epoch: 1, batch: 710/2350, loss: 1.46013,  speed: 0.66 step/s, lr: 1.5085106382978726e-05\n",
      "global step 720, epoch: 1, batch: 720/2350, loss: 1.19559,  speed: 0.66 step/s, lr: 1.5297872340425531e-05\n",
      "global step 730, epoch: 1, batch: 730/2350, loss: 1.37512,  speed: 0.66 step/s, lr: 1.5510638297872340e-05\n",
      "global step 740, epoch: 1, batch: 740/2350, loss: 1.29904,  speed: 0.66 step/s, lr: 1.5723404255319149e-05\n",
      "global step 750, epoch: 1, batch: 750/2350, loss: 1.33004,  speed: 0.66 step/s, lr: 1.5936170212765958e-05\n",
      "global step 760, epoch: 1, batch: 760/2350, loss: 1.00660,  speed: 0.66 step/s, lr: 1.6148936170212767e-05\n",
      "global step 770, epoch: 1, batch: 770/2350, loss: 0.86047,  speed: 0.66 step/s, lr: 1.6361702127659576e-05\n",
      "global step 780, epoch: 1, batch: 780/2350, loss: 1.28850,  speed: 0.65 step/s, lr: 1.6574468085106385e-05\n",
      "global step 790, epoch: 1, batch: 790/2350, loss: 1.34003,  speed: 0.65 step/s, lr: 1.6787234042553194e-05\n",
      "global step 800, epoch: 1, batch: 800/2350, loss: 1.05816,  speed: 0.66 step/s, lr: 1.7000000000000003e-05\n",
      "global step 810, epoch: 1, batch: 810/2350, loss: 1.67555,  speed: 0.66 step/s, lr: 1.7212765957446809e-05\n",
      "global step 820, epoch: 1, batch: 820/2350, loss: 1.18771,  speed: 0.66 step/s, lr: 1.7425531914893618e-05\n",
      "global step 830, epoch: 1, batch: 830/2350, loss: 1.13279,  speed: 0.66 step/s, lr: 1.7638297872340427e-05\n",
      "global step 840, epoch: 1, batch: 840/2350, loss: 1.42764,  speed: 0.66 step/s, lr: 1.7851063829787236e-05\n",
      "global step 850, epoch: 1, batch: 850/2350, loss: 1.24770,  speed: 0.66 step/s, lr: 1.8063829787234045e-05\n",
      "global step 860, epoch: 1, batch: 860/2350, loss: 1.50749,  speed: 0.66 step/s, lr: 1.8276595744680854e-05\n",
      "global step 870, epoch: 1, batch: 870/2350, loss: 1.38199,  speed: 0.66 step/s, lr: 1.8489361702127659e-05\n",
      "global step 880, epoch: 1, batch: 880/2350, loss: 1.25784,  speed: 0.66 step/s, lr: 1.8702127659574468e-05\n",
      "global step 890, epoch: 1, batch: 890/2350, loss: 2.07500,  speed: 0.66 step/s, lr: 1.8914893617021277e-05\n",
      "global step 900, epoch: 1, batch: 900/2350, loss: 0.99452,  speed: 0.66 step/s, lr: 1.9127659574468086e-05\n",
      "global step 910, epoch: 1, batch: 910/2350, loss: 1.15945,  speed: 0.66 step/s, lr: 1.9340425531914892e-05\n",
      "global step 920, epoch: 1, batch: 920/2350, loss: 1.27554,  speed: 0.66 step/s, lr: 1.9553191489361701e-05\n",
      "global step 930, epoch: 1, batch: 930/2350, loss: 2.17368,  speed: 0.66 step/s, lr: 1.9765957446808510e-05\n",
      "global step 940, epoch: 1, batch: 940/2350, loss: 0.89051,  speed: 0.66 step/s, lr: 1.9978723404255319e-05\n",
      "global step 950, epoch: 1, batch: 950/2350, loss: 0.71928,  speed: 0.65 step/s, lr: 2.0191489361702128e-05\n",
      "global step 960, epoch: 1, batch: 960/2350, loss: 0.95775,  speed: 0.66 step/s, lr: 2.0404255319148937e-05\n",
      "global step 970, epoch: 1, batch: 970/2350, loss: 0.96730,  speed: 0.65 step/s, lr: 2.0617021276595746e-05\n",
      "global step 980, epoch: 1, batch: 980/2350, loss: 1.71265,  speed: 0.66 step/s, lr: 2.0829787234042555e-05\n",
      "global step 990, epoch: 1, batch: 990/2350, loss: 1.38568,  speed: 0.66 step/s, lr: 2.1042553191489361e-05\n",
      "global step 1000, epoch: 1, batch: 1000/2350, loss: 1.16882,  speed: 0.66 step/s, lr: 2.1255319148936170e-05\n",
      "Processing example: 1000\n",
      "time per 1000: 24.216672658920288\n",
      "global step: 1000, eval dev Exact Mactch: 0.72618, f1_score: 0.87345\n",
      "save model at global step: 1000, best eval f1_score: 0.87345\n",
      "global step 1010, epoch: 1, batch: 1010/2350, loss: 1.44313,  speed: 0.13 step/s, lr: 2.1468085106382979e-05\n",
      "global step 1020, epoch: 1, batch: 1020/2350, loss: 1.17160,  speed: 0.66 step/s, lr: 2.1680851063829788e-05\n",
      "global step 1030, epoch: 1, batch: 1030/2350, loss: 1.52317,  speed: 0.66 step/s, lr: 2.1893617021276597e-05\n",
      "global step 1040, epoch: 1, batch: 1040/2350, loss: 0.98834,  speed: 0.66 step/s, lr: 2.2106382978723406e-05\n",
      "global step 1050, epoch: 1, batch: 1050/2350, loss: 1.06595,  speed: 0.66 step/s, lr: 2.2319148936170215e-05\n",
      "global step 1060, epoch: 1, batch: 1060/2350, loss: 0.98458,  speed: 0.66 step/s, lr: 2.2531914893617024e-05\n",
      "global step 1070, epoch: 1, batch: 1070/2350, loss: 1.16886,  speed: 0.66 step/s, lr: 2.2744680851063833e-05\n",
      "global step 1080, epoch: 1, batch: 1080/2350, loss: 1.13945,  speed: 0.66 step/s, lr: 2.2957446808510638e-05\n",
      "global step 1090, epoch: 1, batch: 1090/2350, loss: 1.22920,  speed: 0.66 step/s, lr: 2.3170212765957447e-05\n",
      "global step 1100, epoch: 1, batch: 1100/2350, loss: 1.08550,  speed: 0.66 step/s, lr: 2.3382978723404256e-05\n",
      "global step 1110, epoch: 1, batch: 1110/2350, loss: 1.16151,  speed: 0.66 step/s, lr: 2.3595744680851065e-05\n",
      "global step 1120, epoch: 1, batch: 1120/2350, loss: 0.92204,  speed: 0.66 step/s, lr: 2.3808510638297874e-05\n",
      "global step 1130, epoch: 1, batch: 1130/2350, loss: 1.26279,  speed: 0.66 step/s, lr: 2.4021276595744683e-05\n",
      "global step 1140, epoch: 1, batch: 1140/2350, loss: 1.37316,  speed: 0.66 step/s, lr: 2.4234042553191492e-05\n",
      "global step 1150, epoch: 1, batch: 1150/2350, loss: 1.30781,  speed: 0.66 step/s, lr: 2.4446808510638301e-05\n",
      "global step 1160, epoch: 1, batch: 1160/2350, loss: 0.81647,  speed: 0.66 step/s, lr: 2.4659574468085110e-05\n",
      "global step 1170, epoch: 1, batch: 1170/2350, loss: 1.25572,  speed: 0.66 step/s, lr: 2.4872340425531916e-05\n",
      "global step 1180, epoch: 1, batch: 1180/2350, loss: 1.09205,  speed: 0.66 step/s, lr: 2.5085106382978725e-05\n",
      "global step 1190, epoch: 1, batch: 1190/2350, loss: 0.97341,  speed: 0.66 step/s, lr: 2.5297872340425534e-05\n",
      "global step 1200, epoch: 1, batch: 1200/2350, loss: 1.91735,  speed: 0.66 step/s, lr: 2.5510638297872343e-05\n",
      "global step 1210, epoch: 1, batch: 1210/2350, loss: 2.16966,  speed: 0.66 step/s, lr: 2.5723404255319152e-05\n",
      "global step 1220, epoch: 1, batch: 1220/2350, loss: 1.13366,  speed: 0.66 step/s, lr: 2.5936170212765957e-05\n",
      "global step 1230, epoch: 1, batch: 1230/2350, loss: 1.62879,  speed: 0.66 step/s, lr: 2.6148936170212766e-05\n",
      "global step 1240, epoch: 1, batch: 1240/2350, loss: 1.56903,  speed: 0.66 step/s, lr: 2.6361702127659575e-05\n",
      "global step 1250, epoch: 1, batch: 1250/2350, loss: 1.91242,  speed: 0.66 step/s, lr: 2.6574468085106384e-05\n",
      "global step 1260, epoch: 1, batch: 1260/2350, loss: 1.33764,  speed: 0.66 step/s, lr: 2.6787234042553193e-05\n",
      "global step 1270, epoch: 1, batch: 1270/2350, loss: 1.06063,  speed: 0.66 step/s, lr: 2.7000000000000002e-05\n",
      "global step 1280, epoch: 1, batch: 1280/2350, loss: 1.03810,  speed: 0.66 step/s, lr: 2.7212765957446811e-05\n",
      "global step 1290, epoch: 1, batch: 1290/2350, loss: 1.17783,  speed: 0.66 step/s, lr: 2.7425531914893620e-05\n",
      "global step 1300, epoch: 1, batch: 1300/2350, loss: 1.65311,  speed: 0.66 step/s, lr: 2.7638297872340429e-05\n",
      "global step 1310, epoch: 1, batch: 1310/2350, loss: 1.39061,  speed: 0.66 step/s, lr: 2.7851063829787232e-05\n",
      "global step 1320, epoch: 1, batch: 1320/2350, loss: 1.32437,  speed: 0.66 step/s, lr: 2.8063829787234041e-05\n",
      "global step 1330, epoch: 1, batch: 1330/2350, loss: 1.35492,  speed: 0.66 step/s, lr: 2.8276595744680850e-05\n",
      "global step 1340, epoch: 1, batch: 1340/2350, loss: 1.16098,  speed: 0.66 step/s, lr: 2.8489361702127659e-05\n",
      "global step 1350, epoch: 1, batch: 1350/2350, loss: 1.40835,  speed: 0.66 step/s, lr: 2.8702127659574468e-05\n",
      "global step 1360, epoch: 1, batch: 1360/2350, loss: 5.93081,  speed: 0.66 step/s, lr: 2.8914893617021277e-05\n",
      "global step 1370, epoch: 1, batch: 1370/2350, loss: 5.96833,  speed: 0.66 step/s, lr: 2.9127659574468086e-05\n",
      "global step 1380, epoch: 1, batch: 1380/2350, loss: 5.93989,  speed: 0.66 step/s, lr: 2.9340425531914895e-05\n",
      "global step 1390, epoch: 1, batch: 1390/2350, loss: 5.97217,  speed: 0.66 step/s, lr: 2.9553191489361704e-05\n",
      "global step 1400, epoch: 1, batch: 1400/2350, loss: 5.99744,  speed: 0.66 step/s, lr: 2.9765957446808513e-05\n",
      "global step 1410, epoch: 1, batch: 1410/2350, loss: 5.95553,  speed: 0.66 step/s, lr: 2.9978723404255322e-05\n",
      "global step 1420, epoch: 1, batch: 1420/2350, loss: 5.98969,  speed: 0.66 step/s, lr: 3.0191489361702131e-05\n",
      "global step 1430, epoch: 1, batch: 1430/2350, loss: 5.90745,  speed: 0.66 step/s, lr: 3.0404255319148940e-05\n",
      "global step 1440, epoch: 1, batch: 1440/2350, loss: 5.91996,  speed: 0.66 step/s, lr: 3.0617021276595749e-05\n",
      "global step 1450, epoch: 1, batch: 1450/2350, loss: 5.94267,  speed: 0.66 step/s, lr: 3.0829787234042554e-05\n",
      "global step 1460, epoch: 1, batch: 1460/2350, loss: 5.94873,  speed: 0.66 step/s, lr: 3.1042553191489367e-05\n",
      "global step 1470, epoch: 1, batch: 1470/2350, loss: 5.98076,  speed: 0.66 step/s, lr: 3.1255319148936172e-05\n",
      "global step 1480, epoch: 1, batch: 1480/2350, loss: 5.93401,  speed: 0.66 step/s, lr: 3.1468085106382985e-05\n",
      "global step 1490, epoch: 1, batch: 1490/2350, loss: 5.95394,  speed: 0.66 step/s, lr: 3.1680851063829783e-05\n",
      "global step 1500, epoch: 1, batch: 1500/2350, loss: 5.92801,  speed: 0.66 step/s, lr: 3.1893617021276596e-05\n",
      "Processing example: 1000\n",
      "time per 1000: 24.00972557067871\n",
      "global step: 1500, eval dev Exact Mactch: 0.00000, f1_score: 0.06090\n",
      "global step 1510, epoch: 1, batch: 1510/2350, loss: 5.96176,  speed: 0.16 step/s, lr: 3.2106382978723401e-05\n",
      "global step 1520, epoch: 1, batch: 1520/2350, loss: 5.95710,  speed: 0.66 step/s, lr: 3.2319148936170214e-05\n",
      "global step 1530, epoch: 1, batch: 1530/2350, loss: 5.93603,  speed: 0.66 step/s, lr: 3.2531914893617019e-05\n",
      "global step 1540, epoch: 1, batch: 1540/2350, loss: 6.00252,  speed: 0.66 step/s, lr: 3.2744680851063832e-05\n",
      "global step 1550, epoch: 1, batch: 1550/2350, loss: 5.93232,  speed: 0.66 step/s, lr: 3.2957446808510637e-05\n",
      "global step 1560, epoch: 1, batch: 1560/2350, loss: 5.91679,  speed: 0.66 step/s, lr: 3.3170212765957450e-05\n",
      "global step 1570, epoch: 1, batch: 1570/2350, loss: 5.95103,  speed: 0.66 step/s, lr: 3.3382978723404255e-05\n",
      "global step 1580, epoch: 1, batch: 1580/2350, loss: 5.97826,  speed: 0.66 step/s, lr: 3.3595744680851068e-05\n",
      "global step 1590, epoch: 1, batch: 1590/2350, loss: 5.94958,  speed: 0.66 step/s, lr: 3.3808510638297873e-05\n",
      "global step 1600, epoch: 1, batch: 1600/2350, loss: 5.94799,  speed: 0.66 step/s, lr: 3.4021276595744686e-05\n",
      "global step 1610, epoch: 1, batch: 1610/2350, loss: 5.93032,  speed: 0.66 step/s, lr: 3.4234042553191491e-05\n",
      "global step 1620, epoch: 1, batch: 1620/2350, loss: 5.97086,  speed: 0.66 step/s, lr: 3.4446808510638304e-05\n",
      "global step 1630, epoch: 1, batch: 1630/2350, loss: 5.94540,  speed: 0.66 step/s, lr: 3.4659574468085109e-05\n",
      "global step 1640, epoch: 1, batch: 1640/2350, loss: 5.96403,  speed: 0.66 step/s, lr: 3.4872340425531922e-05\n",
      "global step 1650, epoch: 1, batch: 1650/2350, loss: 5.97399,  speed: 0.66 step/s, lr: 3.5085106382978727e-05\n",
      "global step 1660, epoch: 1, batch: 1660/2350, loss: 5.98752,  speed: 0.66 step/s, lr: 3.5297872340425533e-05\n",
      "global step 1670, epoch: 1, batch: 1670/2350, loss: 5.98061,  speed: 0.66 step/s, lr: 3.5510638297872339e-05\n",
      "global step 1680, epoch: 1, batch: 1680/2350, loss: 5.94676,  speed: 0.66 step/s, lr: 3.5723404255319151e-05\n",
      "global step 1690, epoch: 1, batch: 1690/2350, loss: 5.95836,  speed: 0.66 step/s, lr: 3.5936170212765957e-05\n",
      "global step 1700, epoch: 1, batch: 1700/2350, loss: 5.95598,  speed: 0.66 step/s, lr: 3.6148936170212769e-05\n",
      "global step 1710, epoch: 1, batch: 1710/2350, loss: 5.96490,  speed: 0.66 step/s, lr: 3.6361702127659575e-05\n",
      "global step 1720, epoch: 1, batch: 1720/2350, loss: 5.96300,  speed: 0.66 step/s, lr: 3.6574468085106387e-05\n",
      "global step 1730, epoch: 1, batch: 1730/2350, loss: 5.97343,  speed: 0.66 step/s, lr: 3.6787234042553193e-05\n",
      "global step 1740, epoch: 1, batch: 1740/2350, loss: 5.94941,  speed: 0.66 step/s, lr: 3.6999999999999998e-05\n",
      "global step 1750, epoch: 1, batch: 1750/2350, loss: 5.94451,  speed: 0.66 step/s, lr: 3.7212765957446810e-05\n",
      "global step 1760, epoch: 1, batch: 1760/2350, loss: 5.94794,  speed: 0.66 step/s, lr: 3.7425531914893616e-05\n",
      "global step 1770, epoch: 1, batch: 1770/2350, loss: 5.97102,  speed: 0.66 step/s, lr: 3.7638297872340428e-05\n",
      "global step 1780, epoch: 1, batch: 1780/2350, loss: 5.94823,  speed: 0.66 step/s, lr: 3.7851063829787234e-05\n",
      "global step 1790, epoch: 1, batch: 1790/2350, loss: 5.98390,  speed: 0.66 step/s, lr: 3.8063829787234046e-05\n",
      "global step 1800, epoch: 1, batch: 1800/2350, loss: 5.96758,  speed: 0.66 step/s, lr: 3.8276595744680852e-05\n",
      "global step 1810, epoch: 1, batch: 1810/2350, loss: 5.91205,  speed: 0.66 step/s, lr: 3.8489361702127664e-05\n",
      "global step 1820, epoch: 1, batch: 1820/2350, loss: 5.98036,  speed: 0.66 step/s, lr: 3.8702127659574470e-05\n",
      "global step 1830, epoch: 1, batch: 1830/2350, loss: 5.94382,  speed: 0.66 step/s, lr: 3.8914893617021282e-05\n",
      "global step 1840, epoch: 1, batch: 1840/2350, loss: 5.95120,  speed: 0.66 step/s, lr: 3.9127659574468088e-05\n",
      "global step 1850, epoch: 1, batch: 1850/2350, loss: 5.95201,  speed: 0.66 step/s, lr: 3.9340425531914894e-05\n",
      "global step 1860, epoch: 1, batch: 1860/2350, loss: 5.96393,  speed: 0.66 step/s, lr: 3.9553191489361699e-05\n",
      "global step 1870, epoch: 1, batch: 1870/2350, loss: 5.93235,  speed: 0.66 step/s, lr: 3.9765957446808512e-05\n",
      "global step 1880, epoch: 1, batch: 1880/2350, loss: 5.97520,  speed: 0.66 step/s, lr: 3.9978723404255317e-05\n",
      "global step 1890, epoch: 1, batch: 1890/2350, loss: 5.96960,  speed: 0.66 step/s, lr: 4.0191489361702130e-05\n",
      "global step 1900, epoch: 1, batch: 1900/2350, loss: 5.95000,  speed: 0.66 step/s, lr: 4.0404255319148935e-05\n",
      "global step 1910, epoch: 1, batch: 1910/2350, loss: 5.97814,  speed: 0.66 step/s, lr: 4.0617021276595748e-05\n",
      "global step 1920, epoch: 1, batch: 1920/2350, loss: 5.99112,  speed: 0.66 step/s, lr: 4.0829787234042553e-05\n",
      "global step 1930, epoch: 1, batch: 1930/2350, loss: 5.99271,  speed: 0.66 step/s, lr: 4.1042553191489366e-05\n",
      "global step 1940, epoch: 1, batch: 1940/2350, loss: 5.96157,  speed: 0.66 step/s, lr: 4.1255319148936171e-05\n",
      "global step 1950, epoch: 1, batch: 1950/2350, loss: 5.98050,  speed: 0.66 step/s, lr: 4.1468085106382984e-05\n",
      "global step 1960, epoch: 1, batch: 1960/2350, loss: 5.97477,  speed: 0.66 step/s, lr: 4.1680851063829789e-05\n",
      "global step 1970, epoch: 1, batch: 1970/2350, loss: 5.99603,  speed: 0.66 step/s, lr: 4.1893617021276602e-05\n",
      "global step 1980, epoch: 1, batch: 1980/2350, loss: 5.96274,  speed: 0.66 step/s, lr: 4.2106382978723407e-05\n",
      "global step 1990, epoch: 1, batch: 1990/2350, loss: 5.98454,  speed: 0.66 step/s, lr: 4.2319148936170220e-05\n",
      "global step 2000, epoch: 1, batch: 2000/2350, loss: 5.97424,  speed: 0.66 step/s, lr: 4.2531914893617025e-05\n",
      "Processing example: 1000\n",
      "time per 1000: 24.0237398147583\n",
      "global step: 2000, eval dev Exact Mactch: 0.00000, f1_score: 0.02631\n",
      "global step 2010, epoch: 1, batch: 2010/2350, loss: 5.98757,  speed: 0.16 step/s, lr: 4.2744680851063831e-05\n",
      "global step 2020, epoch: 1, batch: 2020/2350, loss: 5.95329,  speed: 0.66 step/s, lr: 4.2957446808510643e-05\n",
      "global step 2030, epoch: 1, batch: 2030/2350, loss: 5.95666,  speed: 0.66 step/s, lr: 4.3170212765957449e-05\n",
      "global step 2040, epoch: 1, batch: 2040/2350, loss: 5.98592,  speed: 0.66 step/s, lr: 4.3382978723404254e-05\n",
      "global step 2050, epoch: 1, batch: 2050/2350, loss: 5.96346,  speed: 0.66 step/s, lr: 4.3595744680851067e-05\n",
      "global step 2060, epoch: 1, batch: 2060/2350, loss: 5.96131,  speed: 0.66 step/s, lr: 4.3808510638297872e-05\n",
      "global step 2070, epoch: 1, batch: 2070/2350, loss: 5.94858,  speed: 0.66 step/s, lr: 4.4021276595744685e-05\n",
      "global step 2080, epoch: 1, batch: 2080/2350, loss: 5.96262,  speed: 0.66 step/s, lr: 4.4234042553191490e-05\n",
      "global step 2090, epoch: 1, batch: 2090/2350, loss: 5.94199,  speed: 0.66 step/s, lr: 4.4446808510638296e-05\n",
      "global step 2100, epoch: 1, batch: 2100/2350, loss: 5.95358,  speed: 0.66 step/s, lr: 4.4659574468085108e-05\n",
      "global step 2110, epoch: 1, batch: 2110/2350, loss: 5.93475,  speed: 0.66 step/s, lr: 4.4872340425531914e-05\n",
      "global step 2120, epoch: 1, batch: 2120/2350, loss: 5.93542,  speed: 0.66 step/s, lr: 4.5085106382978726e-05\n",
      "global step 2130, epoch: 1, batch: 2130/2350, loss: 5.96957,  speed: 0.66 step/s, lr: 4.5297872340425532e-05\n",
      "global step 2140, epoch: 1, batch: 2140/2350, loss: 5.93927,  speed: 0.66 step/s, lr: 4.5510638297872344e-05\n",
      "global step 2150, epoch: 1, batch: 2150/2350, loss: 5.96352,  speed: 0.66 step/s, lr: 4.5723404255319150e-05\n",
      "global step 2160, epoch: 1, batch: 2160/2350, loss: 5.95794,  speed: 0.66 step/s, lr: 4.5936170212765962e-05\n",
      "global step 2170, epoch: 1, batch: 2170/2350, loss: 5.96409,  speed: 0.66 step/s, lr: 4.6148936170212768e-05\n",
      "global step 2180, epoch: 1, batch: 2180/2350, loss: 5.98694,  speed: 0.66 step/s, lr: 4.6361702127659580e-05\n",
      "global step 2190, epoch: 1, batch: 2190/2350, loss: 5.93550,  speed: 0.66 step/s, lr: 4.6574468085106386e-05\n",
      "global step 2200, epoch: 1, batch: 2200/2350, loss: 5.96039,  speed: 0.66 step/s, lr: 4.6787234042553198e-05\n",
      "global step 2210, epoch: 1, batch: 2210/2350, loss: 5.94778,  speed: 0.66 step/s, lr: 4.6999999999999997e-05\n",
      "global step 2220, epoch: 1, batch: 2220/2350, loss: 5.96103,  speed: 0.66 step/s, lr: 4.7212765957446810e-05\n",
      "global step 2230, epoch: 1, batch: 2230/2350, loss: 5.98093,  speed: 0.66 step/s, lr: 4.7425531914893615e-05\n",
      "global step 2240, epoch: 1, batch: 2240/2350, loss: 5.96445,  speed: 0.66 step/s, lr: 4.7638297872340428e-05\n",
      "global step 2250, epoch: 1, batch: 2250/2350, loss: 5.95590,  speed: 0.66 step/s, lr: 4.7851063829787233e-05\n",
      "global step 2260, epoch: 1, batch: 2260/2350, loss: 5.98392,  speed: 0.66 step/s, lr: 4.8063829787234046e-05\n",
      "global step 2270, epoch: 1, batch: 2270/2350, loss: 5.94332,  speed: 0.66 step/s, lr: 4.8276595744680851e-05\n",
      "global step 2280, epoch: 1, batch: 2280/2350, loss: 5.95402,  speed: 0.66 step/s, lr: 4.8489361702127664e-05\n",
      "global step 2290, epoch: 1, batch: 2290/2350, loss: 5.98344,  speed: 0.66 step/s, lr: 4.8702127659574469e-05\n",
      "global step 2300, epoch: 1, batch: 2300/2350, loss: 5.93775,  speed: 0.66 step/s, lr: 4.8914893617021282e-05\n",
      "global step 2310, epoch: 1, batch: 2310/2350, loss: 5.95630,  speed: 0.66 step/s, lr: 4.9127659574468087e-05\n",
      "global step 2320, epoch: 1, batch: 2320/2350, loss: 5.95290,  speed: 0.66 step/s, lr: 4.9340425531914900e-05\n",
      "global step 2330, epoch: 1, batch: 2330/2350, loss: 5.95877,  speed: 0.66 step/s, lr: 4.9553191489361705e-05\n",
      "global step 2340, epoch: 1, batch: 2340/2350, loss: 5.93875,  speed: 0.66 step/s, lr: 4.9765957446808518e-05\n",
      "global step 2350, epoch: 1, batch: 2350/2350, loss: 5.93782,  speed: 0.68 step/s, lr: 4.9978723404255323e-05\n",
      "global step 2360, epoch: 2, batch: 10/2350, loss: 5.93895,  speed: 0.69 step/s, lr: 4.9978723404255323e-05\n",
      "global step 2370, epoch: 2, batch: 20/2350, loss: 5.94763,  speed: 0.66 step/s, lr: 4.9955082742316788e-05\n",
      "global step 2380, epoch: 2, batch: 30/2350, loss: 5.97626,  speed: 0.66 step/s, lr: 4.9931442080378252e-05\n",
      "global step 2390, epoch: 2, batch: 40/2350, loss: 5.95746,  speed: 0.66 step/s, lr: 4.9907801418439724e-05\n",
      "global step 2400, epoch: 2, batch: 50/2350, loss: 5.97698,  speed: 0.66 step/s, lr: 4.9884160756501188e-05\n",
      "global step 2410, epoch: 2, batch: 60/2350, loss: 5.95606,  speed: 0.66 step/s, lr: 4.9860520094562653e-05\n",
      "global step 2420, epoch: 2, batch: 70/2350, loss: 5.96017,  speed: 0.66 step/s, lr: 4.9836879432624117e-05\n",
      "global step 2430, epoch: 2, batch: 80/2350, loss: 5.94313,  speed: 0.66 step/s, lr: 4.9813238770685582e-05\n",
      "global step 2440, epoch: 2, batch: 90/2350, loss: 5.90891,  speed: 0.66 step/s, lr: 4.9789598108747046e-05\n",
      "global step 2450, epoch: 2, batch: 100/2350, loss: 5.95851,  speed: 0.66 step/s, lr: 4.9765957446808518e-05\n",
      "global step 2460, epoch: 2, batch: 110/2350, loss: 5.96317,  speed: 0.66 step/s, lr: 4.9742316784869982e-05\n",
      "global step 2470, epoch: 2, batch: 120/2350, loss: 5.96103,  speed: 0.66 step/s, lr: 4.9718676122931447e-05\n",
      "global step 2480, epoch: 2, batch: 130/2350, loss: 5.95592,  speed: 0.66 step/s, lr: 4.9695035460992911e-05\n",
      "global step 2490, epoch: 2, batch: 140/2350, loss: 5.94493,  speed: 0.66 step/s, lr: 4.9671394799054376e-05\n",
      "global step 2500, epoch: 2, batch: 150/2350, loss: 5.93675,  speed: 0.66 step/s, lr: 4.9647754137115840e-05\n",
      "Processing example: 1000\n",
      "time per 1000: 24.01472282409668\n",
      "global step: 2500, eval dev Exact Mactch: 0.00000, f1_score: 0.10435\n",
      "global step 2510, epoch: 2, batch: 160/2350, loss: 6.00382,  speed: 0.16 step/s, lr: 4.9624113475177312e-05\n",
      "global step 2520, epoch: 2, batch: 170/2350, loss: 5.95003,  speed: 0.66 step/s, lr: 4.9600472813238776e-05\n",
      "global step 2530, epoch: 2, batch: 180/2350, loss: 5.99130,  speed: 0.66 step/s, lr: 4.9576832151300241e-05\n",
      "global step 2540, epoch: 2, batch: 190/2350, loss: 5.96940,  speed: 0.66 step/s, lr: 4.9553191489361705e-05\n",
      "global step 2550, epoch: 2, batch: 200/2350, loss: 5.95930,  speed: 0.66 step/s, lr: 4.9529550827423170e-05\n",
      "global step 2560, epoch: 2, batch: 210/2350, loss: 5.93431,  speed: 0.66 step/s, lr: 4.9505910165484634e-05\n",
      "global step 2570, epoch: 2, batch: 220/2350, loss: 5.97149,  speed: 0.66 step/s, lr: 4.9482269503546106e-05\n",
      "global step 2580, epoch: 2, batch: 230/2350, loss: 5.98580,  speed: 0.66 step/s, lr: 4.9458628841607570e-05\n",
      "global step 2590, epoch: 2, batch: 240/2350, loss: 5.96277,  speed: 0.66 step/s, lr: 4.9434988179669035e-05\n",
      "global step 2600, epoch: 2, batch: 250/2350, loss: 5.96844,  speed: 0.66 step/s, lr: 4.9411347517730499e-05\n",
      "global step 2610, epoch: 2, batch: 260/2350, loss: 5.92351,  speed: 0.66 step/s, lr: 4.9387706855791964e-05\n",
      "global step 2620, epoch: 2, batch: 270/2350, loss: 5.96969,  speed: 0.66 step/s, lr: 4.9364066193853428e-05\n",
      "global step 2630, epoch: 2, batch: 280/2350, loss: 5.92512,  speed: 0.66 step/s, lr: 4.9340425531914900e-05\n",
      "global step 2640, epoch: 2, batch: 290/2350, loss: 5.94701,  speed: 0.66 step/s, lr: 4.9316784869976364e-05\n",
      "global step 2650, epoch: 2, batch: 300/2350, loss: 5.94859,  speed: 0.66 step/s, lr: 4.9293144208037829e-05\n",
      "global step 2660, epoch: 2, batch: 310/2350, loss: 5.94246,  speed: 0.66 step/s, lr: 4.9269503546099293e-05\n",
      "global step 2670, epoch: 2, batch: 320/2350, loss: 5.96033,  speed: 0.66 step/s, lr: 4.9245862884160758e-05\n",
      "global step 2680, epoch: 2, batch: 330/2350, loss: 5.94560,  speed: 0.66 step/s, lr: 4.9222222222222222e-05\n",
      "global step 2690, epoch: 2, batch: 340/2350, loss: 5.93146,  speed: 0.66 step/s, lr: 4.9198581560283694e-05\n",
      "global step 2700, epoch: 2, batch: 350/2350, loss: 5.95774,  speed: 0.66 step/s, lr: 4.9174940898345158e-05\n",
      "global step 2710, epoch: 2, batch: 360/2350, loss: 5.94312,  speed: 0.66 step/s, lr: 4.9151300236406623e-05\n",
      "global step 2730, epoch: 2, batch: 380/2350, loss: 5.94115,  speed: 0.66 step/s, lr: 4.9104018912529552e-05\n",
      "global step 2740, epoch: 2, batch: 390/2350, loss: 5.94412,  speed: 0.66 step/s, lr: 4.9080378250591016e-05\n",
      "global step 2750, epoch: 2, batch: 400/2350, loss: 5.97631,  speed: 0.66 step/s, lr: 4.9056737588652488e-05\n",
      "global step 2760, epoch: 2, batch: 410/2350, loss: 5.95186,  speed: 0.66 step/s, lr: 4.9033096926713952e-05\n",
      "global step 2770, epoch: 2, batch: 420/2350, loss: 5.94256,  speed: 0.66 step/s, lr: 4.9009456264775417e-05\n",
      "global step 2780, epoch: 2, batch: 430/2350, loss: 5.97436,  speed: 0.66 step/s, lr: 4.8985815602836881e-05\n",
      "global step 2790, epoch: 2, batch: 440/2350, loss: 5.96983,  speed: 0.66 step/s, lr: 4.8962174940898346e-05\n",
      "global step 2800, epoch: 2, batch: 450/2350, loss: 5.93824,  speed: 0.66 step/s, lr: 4.8938534278959810e-05\n",
      "global step 2810, epoch: 2, batch: 460/2350, loss: 5.94433,  speed: 0.66 step/s, lr: 4.8914893617021282e-05\n",
      "global step 2820, epoch: 2, batch: 470/2350, loss: 5.97867,  speed: 0.66 step/s, lr: 4.8891252955082746e-05\n",
      "global step 2830, epoch: 2, batch: 480/2350, loss: 5.96599,  speed: 0.66 step/s, lr: 4.8867612293144211e-05\n",
      "global step 2840, epoch: 2, batch: 490/2350, loss: 5.95992,  speed: 0.66 step/s, lr: 4.8843971631205675e-05\n",
      "global step 2850, epoch: 2, batch: 500/2350, loss: 5.95436,  speed: 0.66 step/s, lr: 4.8820330969267140e-05\n",
      "global step 2860, epoch: 2, batch: 510/2350, loss: 5.94990,  speed: 0.66 step/s, lr: 4.8796690307328604e-05\n",
      "global step 2870, epoch: 2, batch: 520/2350, loss: 5.97505,  speed: 0.66 step/s, lr: 4.8773049645390076e-05\n",
      "global step 2880, epoch: 2, batch: 530/2350, loss: 5.95421,  speed: 0.66 step/s, lr: 4.8749408983451540e-05\n",
      "global step 2890, epoch: 2, batch: 540/2350, loss: 5.92657,  speed: 0.66 step/s, lr: 4.8725768321513005e-05\n",
      "global step 2900, epoch: 2, batch: 550/2350, loss: 5.98020,  speed: 0.66 step/s, lr: 4.8702127659574469e-05\n",
      "global step 2910, epoch: 2, batch: 560/2350, loss: 5.93939,  speed: 0.66 step/s, lr: 4.8678486997635934e-05\n",
      "global step 2920, epoch: 2, batch: 570/2350, loss: 5.97518,  speed: 0.66 step/s, lr: 4.8654846335697398e-05\n",
      "global step 2930, epoch: 2, batch: 580/2350, loss: 5.99095,  speed: 0.66 step/s, lr: 4.8631205673758870e-05\n",
      "global step 2940, epoch: 2, batch: 590/2350, loss: 5.98522,  speed: 0.66 step/s, lr: 4.8607565011820334e-05\n",
      "global step 2950, epoch: 2, batch: 600/2350, loss: 5.97142,  speed: 0.66 step/s, lr: 4.8583924349881799e-05\n",
      "global step 2960, epoch: 2, batch: 610/2350, loss: 5.91431,  speed: 0.66 step/s, lr: 4.8560283687943263e-05\n",
      "global step 2970, epoch: 2, batch: 620/2350, loss: 5.94792,  speed: 0.66 step/s, lr: 4.8536643026004728e-05\n",
      "global step 2980, epoch: 2, batch: 630/2350, loss: 5.94841,  speed: 0.66 step/s, lr: 4.8513002364066199e-05\n",
      "global step 2990, epoch: 2, batch: 640/2350, loss: 5.95885,  speed: 0.66 step/s, lr: 4.8489361702127664e-05\n",
      "global step 3000, epoch: 2, batch: 650/2350, loss: 5.94808,  speed: 0.66 step/s, lr: 4.8465721040189128e-05\n",
      "Processing example: 1000\n",
      "time per 1000: 24.01067852973938\n",
      "global step: 3000, eval dev Exact Mactch: 0.00000, f1_score: 0.05688\n",
      "global step 3010, epoch: 2, batch: 660/2350, loss: 5.97407,  speed: 0.16 step/s, lr: 4.8442080378250593e-05\n",
      "global step 3020, epoch: 2, batch: 670/2350, loss: 5.96021,  speed: 0.66 step/s, lr: 4.8418439716312057e-05\n",
      "global step 3030, epoch: 2, batch: 680/2350, loss: 5.94314,  speed: 0.66 step/s, lr: 4.8394799054373522e-05\n",
      "global step 3040, epoch: 2, batch: 690/2350, loss: 5.96534,  speed: 0.66 step/s, lr: 4.8371158392434993e-05\n",
      "global step 3050, epoch: 2, batch: 700/2350, loss: 5.93260,  speed: 0.66 step/s, lr: 4.8347517730496458e-05\n",
      "global step 3060, epoch: 2, batch: 710/2350, loss: 5.95404,  speed: 0.66 step/s, lr: 4.8323877068557922e-05\n",
      "global step 3070, epoch: 2, batch: 720/2350, loss: 5.95753,  speed: 0.66 step/s, lr: 4.8300236406619387e-05\n",
      "global step 3080, epoch: 2, batch: 730/2350, loss: 5.98252,  speed: 0.66 step/s, lr: 4.8276595744680851e-05\n",
      "global step 3090, epoch: 2, batch: 740/2350, loss: 5.95194,  speed: 0.66 step/s, lr: 4.8252955082742316e-05\n",
      "global step 3100, epoch: 2, batch: 750/2350, loss: 5.96445,  speed: 0.66 step/s, lr: 4.8229314420803787e-05\n",
      "global step 3110, epoch: 2, batch: 760/2350, loss: 5.95479,  speed: 0.66 step/s, lr: 4.8205673758865252e-05\n",
      "global step 3120, epoch: 2, batch: 770/2350, loss: 5.93559,  speed: 0.66 step/s, lr: 4.8182033096926716e-05\n",
      "global step 3130, epoch: 2, batch: 780/2350, loss: 5.94384,  speed: 0.66 step/s, lr: 4.8158392434988181e-05\n",
      "global step 3140, epoch: 2, batch: 790/2350, loss: 5.94934,  speed: 0.66 step/s, lr: 4.8134751773049645e-05\n",
      "global step 3150, epoch: 2, batch: 800/2350, loss: 5.98739,  speed: 0.66 step/s, lr: 4.8111111111111110e-05\n",
      "global step 3160, epoch: 2, batch: 810/2350, loss: 5.98310,  speed: 0.66 step/s, lr: 4.8087470449172581e-05\n",
      "global step 3170, epoch: 2, batch: 820/2350, loss: 5.95612,  speed: 0.66 step/s, lr: 4.8063829787234046e-05\n",
      "global step 3180, epoch: 2, batch: 830/2350, loss: 5.94731,  speed: 0.66 step/s, lr: 4.8040189125295510e-05\n",
      "global step 3190, epoch: 2, batch: 840/2350, loss: 5.95004,  speed: 0.66 step/s, lr: 4.8016548463356975e-05\n",
      "global step 3200, epoch: 2, batch: 850/2350, loss: 5.93018,  speed: 0.66 step/s, lr: 4.7992907801418439e-05\n",
      "global step 3210, epoch: 2, batch: 860/2350, loss: 5.98153,  speed: 0.66 step/s, lr: 4.7969267139479904e-05\n",
      "global step 3220, epoch: 2, batch: 870/2350, loss: 5.93770,  speed: 0.66 step/s, lr: 4.7945626477541375e-05\n",
      "global step 3230, epoch: 2, batch: 880/2350, loss: 5.98306,  speed: 0.66 step/s, lr: 4.7921985815602840e-05\n",
      "global step 3240, epoch: 2, batch: 890/2350, loss: 5.96199,  speed: 0.66 step/s, lr: 4.7898345153664304e-05\n",
      "global step 3250, epoch: 2, batch: 900/2350, loss: 5.96702,  speed: 0.66 step/s, lr: 4.7874704491725769e-05\n",
      "global step 3260, epoch: 2, batch: 910/2350, loss: 5.92744,  speed: 0.66 step/s, lr: 4.7851063829787233e-05\n",
      "global step 3270, epoch: 2, batch: 920/2350, loss: 5.96546,  speed: 0.66 step/s, lr: 4.7827423167848698e-05\n",
      "global step 3280, epoch: 2, batch: 930/2350, loss: 5.97210,  speed: 0.66 step/s, lr: 4.7803782505910169e-05\n",
      "global step 3290, epoch: 2, batch: 940/2350, loss: 5.93746,  speed: 0.66 step/s, lr: 4.7780141843971634e-05\n",
      "global step 3300, epoch: 2, batch: 950/2350, loss: 5.96559,  speed: 0.66 step/s, lr: 4.7756501182033098e-05\n",
      "global step 3310, epoch: 2, batch: 960/2350, loss: 5.93647,  speed: 0.66 step/s, lr: 4.7732860520094563e-05\n",
      "global step 3320, epoch: 2, batch: 970/2350, loss: 5.98158,  speed: 0.66 step/s, lr: 4.7709219858156027e-05\n",
      "global step 3330, epoch: 2, batch: 980/2350, loss: 5.96085,  speed: 0.66 step/s, lr: 4.7685579196217492e-05\n",
      "global step 3340, epoch: 2, batch: 990/2350, loss: 5.95339,  speed: 0.66 step/s, lr: 4.7661938534278963e-05\n",
      "global step 3350, epoch: 2, batch: 1000/2350, loss: 5.93601,  speed: 0.66 step/s, lr: 4.7638297872340428e-05\n",
      "global step 3360, epoch: 2, batch: 1010/2350, loss: 5.94613,  speed: 0.66 step/s, lr: 4.7614657210401892e-05\n",
      "global step 3370, epoch: 2, batch: 1020/2350, loss: 5.95067,  speed: 0.66 step/s, lr: 4.7591016548463357e-05\n",
      "global step 3380, epoch: 2, batch: 1030/2350, loss: 5.93731,  speed: 0.66 step/s, lr: 4.7567375886524821e-05\n",
      "global step 3390, epoch: 2, batch: 1040/2350, loss: 5.96347,  speed: 0.66 step/s, lr: 4.7543735224586286e-05\n",
      "global step 3400, epoch: 2, batch: 1050/2350, loss: 5.95022,  speed: 0.66 step/s, lr: 4.7520094562647757e-05\n",
      "global step 3410, epoch: 2, batch: 1060/2350, loss: 5.96536,  speed: 0.66 step/s, lr: 4.7496453900709222e-05\n",
      "global step 3420, epoch: 2, batch: 1070/2350, loss: 5.92876,  speed: 0.66 step/s, lr: 4.7472813238770686e-05\n",
      "global step 3430, epoch: 2, batch: 1080/2350, loss: 5.96640,  speed: 0.66 step/s, lr: 4.7449172576832151e-05\n",
      "global step 3440, epoch: 2, batch: 1090/2350, loss: 5.96375,  speed: 0.66 step/s, lr: 4.7425531914893615e-05\n",
      "global step 3450, epoch: 2, batch: 1100/2350, loss: 5.94891,  speed: 0.66 step/s, lr: 4.7401891252955080e-05\n",
      "global step 3460, epoch: 2, batch: 1110/2350, loss: 5.96041,  speed: 0.66 step/s, lr: 4.7378250591016551e-05\n",
      "global step 3470, epoch: 2, batch: 1120/2350, loss: 5.96238,  speed: 0.66 step/s, lr: 4.7354609929078016e-05\n",
      "global step 3480, epoch: 2, batch: 1130/2350, loss: 5.95578,  speed: 0.66 step/s, lr: 4.7330969267139480e-05\n",
      "global step 3490, epoch: 2, batch: 1140/2350, loss: 5.94256,  speed: 0.66 step/s, lr: 4.7307328605200945e-05\n",
      "global step 3500, epoch: 2, batch: 1150/2350, loss: 5.95370,  speed: 0.66 step/s, lr: 4.7283687943262409e-05\n",
      "Processing example: 1000\n",
      "time per 1000: 24.012502670288086\n",
      "global step: 3500, eval dev Exact Mactch: 0.00000, f1_score: 0.09127\n",
      "global step 3510, epoch: 2, batch: 1160/2350, loss: 5.96569,  speed: 0.16 step/s, lr: 4.7260047281323874e-05\n",
      "global step 3520, epoch: 2, batch: 1170/2350, loss: 5.96386,  speed: 0.66 step/s, lr: 4.7236406619385345e-05\n",
      "global step 3530, epoch: 2, batch: 1180/2350, loss: 5.95245,  speed: 0.66 step/s, lr: 4.7212765957446810e-05\n",
      "global step 3540, epoch: 2, batch: 1190/2350, loss: 5.93925,  speed: 0.66 step/s, lr: 4.7189125295508274e-05\n",
      "global step 3550, epoch: 2, batch: 1200/2350, loss: 5.95951,  speed: 0.66 step/s, lr: 4.7165484633569739e-05\n",
      "global step 3560, epoch: 2, batch: 1210/2350, loss: 5.92311,  speed: 0.66 step/s, lr: 4.7141843971631203e-05\n",
      "global step 3570, epoch: 2, batch: 1220/2350, loss: 5.95951,  speed: 0.66 step/s, lr: 4.7118203309692668e-05\n",
      "global step 3580, epoch: 2, batch: 1230/2350, loss: 5.94376,  speed: 0.66 step/s, lr: 4.7094562647754139e-05\n",
      "global step 3590, epoch: 2, batch: 1240/2350, loss: 5.96408,  speed: 0.66 step/s, lr: 4.7070921985815604e-05\n",
      "global step 3600, epoch: 2, batch: 1250/2350, loss: 5.94961,  speed: 0.66 step/s, lr: 4.7047281323877068e-05\n",
      "global step 3610, epoch: 2, batch: 1260/2350, loss: 5.95457,  speed: 0.66 step/s, lr: 4.7023640661938533e-05\n",
      "global step 3620, epoch: 2, batch: 1270/2350, loss: 5.96397,  speed: 0.66 step/s, lr: 4.6999999999999997e-05\n",
      "global step 3630, epoch: 2, batch: 1280/2350, loss: 5.93825,  speed: 0.66 step/s, lr: 4.6976359338061469e-05\n",
      "global step 3640, epoch: 2, batch: 1290/2350, loss: 5.97258,  speed: 0.66 step/s, lr: 4.6952718676122933e-05\n",
      "global step 3650, epoch: 2, batch: 1300/2350, loss: 5.94787,  speed: 0.66 step/s, lr: 4.6929078014184398e-05\n",
      "global step 3660, epoch: 2, batch: 1310/2350, loss: 5.94969,  speed: 0.66 step/s, lr: 4.6905437352245869e-05\n",
      "global step 3670, epoch: 2, batch: 1320/2350, loss: 5.96700,  speed: 0.66 step/s, lr: 4.6881796690307333e-05\n",
      "global step 3680, epoch: 2, batch: 1330/2350, loss: 5.95025,  speed: 0.66 step/s, lr: 4.6858156028368798e-05\n",
      "global step 3690, epoch: 2, batch: 1340/2350, loss: 5.91358,  speed: 0.66 step/s, lr: 4.6834515366430263e-05\n",
      "global step 3700, epoch: 2, batch: 1350/2350, loss: 5.94344,  speed: 0.66 step/s, lr: 4.6810874704491734e-05\n",
      "global step 3710, epoch: 2, batch: 1360/2350, loss: 5.94810,  speed: 0.66 step/s, lr: 4.6787234042553198e-05\n",
      "global step 3720, epoch: 2, batch: 1370/2350, loss: 5.92764,  speed: 0.66 step/s, lr: 4.6763593380614663e-05\n",
      "global step 3730, epoch: 2, batch: 1380/2350, loss: 5.97312,  speed: 0.66 step/s, lr: 4.6739952718676127e-05\n",
      "global step 3740, epoch: 2, batch: 1390/2350, loss: 5.94984,  speed: 0.66 step/s, lr: 4.6716312056737592e-05\n",
      "global step 3750, epoch: 2, batch: 1400/2350, loss: 5.94153,  speed: 0.66 step/s, lr: 4.6692671394799057e-05\n",
      "global step 3760, epoch: 2, batch: 1410/2350, loss: 5.95197,  speed: 0.66 step/s, lr: 4.6669030732860528e-05\n",
      "global step 3770, epoch: 2, batch: 1420/2350, loss: 5.96231,  speed: 0.66 step/s, lr: 4.6645390070921992e-05\n",
      "global step 3780, epoch: 2, batch: 1430/2350, loss: 5.91200,  speed: 0.66 step/s, lr: 4.6621749408983457e-05\n",
      "global step 3790, epoch: 2, batch: 1440/2350, loss: 5.93859,  speed: 0.66 step/s, lr: 4.6598108747044921e-05\n",
      "global step 3800, epoch: 2, batch: 1450/2350, loss: 5.94704,  speed: 0.66 step/s, lr: 4.6574468085106386e-05\n",
      "global step 3810, epoch: 2, batch: 1460/2350, loss: 5.93715,  speed: 0.66 step/s, lr: 4.6550827423167851e-05\n",
      "global step 3820, epoch: 2, batch: 1470/2350, loss: 5.96859,  speed: 0.66 step/s, lr: 4.6527186761229322e-05\n",
      "global step 3830, epoch: 2, batch: 1480/2350, loss: 5.96538,  speed: 0.66 step/s, lr: 4.6503546099290786e-05\n",
      "global step 3840, epoch: 2, batch: 1490/2350, loss: 5.93464,  speed: 0.66 step/s, lr: 4.6479905437352251e-05\n",
      "global step 3850, epoch: 2, batch: 1500/2350, loss: 5.97654,  speed: 0.66 step/s, lr: 4.6456264775413715e-05\n",
      "global step 3860, epoch: 2, batch: 1510/2350, loss: 5.95200,  speed: 0.66 step/s, lr: 4.6432624113475180e-05\n",
      "global step 3870, epoch: 2, batch: 1520/2350, loss: 5.92890,  speed: 0.66 step/s, lr: 4.6408983451536645e-05\n",
      "global step 3880, epoch: 2, batch: 1530/2350, loss: 5.93596,  speed: 0.66 step/s, lr: 4.6385342789598116e-05\n",
      "global step 3890, epoch: 2, batch: 1540/2350, loss: 5.95826,  speed: 0.66 step/s, lr: 4.6361702127659580e-05\n",
      "global step 3900, epoch: 2, batch: 1550/2350, loss: 5.95161,  speed: 0.66 step/s, lr: 4.6338061465721045e-05\n",
      "global step 3910, epoch: 2, batch: 1560/2350, loss: 5.97462,  speed: 0.66 step/s, lr: 4.6314420803782509e-05\n",
      "global step 3920, epoch: 2, batch: 1570/2350, loss: 5.97829,  speed: 0.66 step/s, lr: 4.6290780141843974e-05\n",
      "global step 3930, epoch: 2, batch: 1580/2350, loss: 5.93734,  speed: 0.66 step/s, lr: 4.6267139479905439e-05\n",
      "global step 3940, epoch: 2, batch: 1590/2350, loss: 5.95459,  speed: 0.66 step/s, lr: 4.6243498817966910e-05\n",
      "global step 3950, epoch: 2, batch: 1600/2350, loss: 5.94280,  speed: 0.66 step/s, lr: 4.6219858156028374e-05\n",
      "global step 3960, epoch: 2, batch: 1610/2350, loss: 5.95889,  speed: 0.66 step/s, lr: 4.6196217494089839e-05\n",
      "global step 3970, epoch: 2, batch: 1620/2350, loss: 5.97354,  speed: 0.66 step/s, lr: 4.6172576832151303e-05\n",
      "global step 3980, epoch: 2, batch: 1630/2350, loss: 5.91980,  speed: 0.66 step/s, lr: 4.6148936170212768e-05\n",
      "global step 3990, epoch: 2, batch: 1640/2350, loss: 5.95487,  speed: 0.66 step/s, lr: 4.6125295508274233e-05\n",
      "global step 4000, epoch: 2, batch: 1650/2350, loss: 5.95695,  speed: 0.66 step/s, lr: 4.6101654846335704e-05\n",
      "Processing example: 1000\n",
      "time per 1000: 23.991920948028564\n",
      "global step: 4000, eval dev Exact Mactch: 0.00000, f1_score: 0.10584\n",
      "global step 4010, epoch: 2, batch: 1660/2350, loss: 5.96957,  speed: 0.16 step/s, lr: 4.6078014184397168e-05\n",
      "global step 4020, epoch: 2, batch: 1670/2350, loss: 5.94098,  speed: 0.66 step/s, lr: 4.6054373522458633e-05\n",
      "global step 4030, epoch: 2, batch: 1680/2350, loss: 5.93732,  speed: 0.66 step/s, lr: 4.6030732860520097e-05\n",
      "global step 4050, epoch: 2, batch: 1700/2350, loss: 5.95343,  speed: 0.66 step/s, lr: 4.5983451536643027e-05\n",
      "global step 4060, epoch: 2, batch: 1710/2350, loss: 5.96542,  speed: 0.66 step/s, lr: 4.5959810874704498e-05\n",
      "global step 4070, epoch: 2, batch: 1720/2350, loss: 5.96669,  speed: 0.66 step/s, lr: 4.5936170212765962e-05\n",
      "global step 4080, epoch: 2, batch: 1730/2350, loss: 5.92418,  speed: 0.66 step/s, lr: 4.5912529550827427e-05\n",
      "global step 4090, epoch: 2, batch: 1740/2350, loss: 5.93816,  speed: 0.66 step/s, lr: 4.5888888888888891e-05\n",
      "global step 4100, epoch: 2, batch: 1750/2350, loss: 5.96350,  speed: 0.66 step/s, lr: 4.5865248226950356e-05\n",
      "global step 4110, epoch: 2, batch: 1760/2350, loss: 5.98356,  speed: 0.66 step/s, lr: 4.5841607565011821e-05\n",
      "global step 4120, epoch: 2, batch: 1770/2350, loss: 5.95077,  speed: 0.66 step/s, lr: 4.5817966903073292e-05\n",
      "global step 4130, epoch: 2, batch: 1780/2350, loss: 5.93201,  speed: 0.66 step/s, lr: 4.5794326241134756e-05\n",
      "global step 4140, epoch: 2, batch: 1790/2350, loss: 5.94522,  speed: 0.66 step/s, lr: 4.5770685579196221e-05\n",
      "global step 4150, epoch: 2, batch: 1800/2350, loss: 5.94684,  speed: 0.66 step/s, lr: 4.5747044917257685e-05\n",
      "global step 4160, epoch: 2, batch: 1810/2350, loss: 5.95205,  speed: 0.66 step/s, lr: 4.5723404255319150e-05\n",
      "global step 4170, epoch: 2, batch: 1820/2350, loss: 5.95483,  speed: 0.66 step/s, lr: 4.5699763593380615e-05\n",
      "global step 4180, epoch: 2, batch: 1830/2350, loss: 5.95404,  speed: 0.66 step/s, lr: 4.5676122931442086e-05\n",
      "global step 4190, epoch: 2, batch: 1840/2350, loss: 5.96365,  speed: 0.66 step/s, lr: 4.5652482269503550e-05\n",
      "global step 4200, epoch: 2, batch: 1850/2350, loss: 5.96320,  speed: 0.66 step/s, lr: 4.5628841607565015e-05\n",
      "global step 4210, epoch: 2, batch: 1860/2350, loss: 5.94619,  speed: 0.66 step/s, lr: 4.5605200945626479e-05\n",
      "global step 4220, epoch: 2, batch: 1870/2350, loss: 5.95066,  speed: 0.66 step/s, lr: 4.5581560283687944e-05\n",
      "global step 4230, epoch: 2, batch: 1880/2350, loss: 5.94521,  speed: 0.66 step/s, lr: 4.5557919621749415e-05\n",
      "global step 4240, epoch: 2, batch: 1890/2350, loss: 5.97677,  speed: 0.66 step/s, lr: 4.5534278959810880e-05\n",
      "global step 4250, epoch: 2, batch: 1900/2350, loss: 5.97506,  speed: 0.66 step/s, lr: 4.5510638297872344e-05\n",
      "global step 4260, epoch: 2, batch: 1910/2350, loss: 5.97127,  speed: 0.66 step/s, lr: 4.5486997635933809e-05\n",
      "global step 4270, epoch: 2, batch: 1920/2350, loss: 5.94953,  speed: 0.66 step/s, lr: 4.5463356973995274e-05\n",
      "global step 4280, epoch: 2, batch: 1930/2350, loss: 5.93952,  speed: 0.66 step/s, lr: 4.5439716312056738e-05\n",
      "global step 4290, epoch: 2, batch: 1940/2350, loss: 5.94440,  speed: 0.66 step/s, lr: 4.5416075650118209e-05\n",
      "global step 4300, epoch: 2, batch: 1950/2350, loss: 5.93482,  speed: 0.66 step/s, lr: 4.5392434988179674e-05\n",
      "global step 4310, epoch: 2, batch: 1960/2350, loss: 5.94157,  speed: 0.66 step/s, lr: 4.5368794326241138e-05\n",
      "global step 4320, epoch: 2, batch: 1970/2350, loss: 5.94969,  speed: 0.66 step/s, lr: 4.5345153664302603e-05\n",
      "global step 4330, epoch: 2, batch: 1980/2350, loss: 5.94438,  speed: 0.66 step/s, lr: 4.5321513002364068e-05\n",
      "global step 4340, epoch: 2, batch: 1990/2350, loss: 5.94792,  speed: 0.66 step/s, lr: 4.5297872340425532e-05\n",
      "global step 4350, epoch: 2, batch: 2000/2350, loss: 5.94480,  speed: 0.66 step/s, lr: 4.5274231678487003e-05\n",
      "global step 4360, epoch: 2, batch: 2010/2350, loss: 5.94966,  speed: 0.66 step/s, lr: 4.5250591016548468e-05\n",
      "global step 4370, epoch: 2, batch: 2020/2350, loss: 5.94107,  speed: 0.66 step/s, lr: 4.5226950354609932e-05\n",
      "global step 4380, epoch: 2, batch: 2030/2350, loss: 5.96869,  speed: 0.66 step/s, lr: 4.5203309692671397e-05\n",
      "global step 4390, epoch: 2, batch: 2040/2350, loss: 5.93234,  speed: 0.66 step/s, lr: 4.5179669030732862e-05\n",
      "global step 4400, epoch: 2, batch: 2050/2350, loss: 5.96015,  speed: 0.66 step/s, lr: 4.5156028368794326e-05\n",
      "global step 4410, epoch: 2, batch: 2060/2350, loss: 5.94456,  speed: 0.66 step/s, lr: 4.5132387706855797e-05\n",
      "global step 4420, epoch: 2, batch: 2070/2350, loss: 5.94241,  speed: 0.66 step/s, lr: 4.5108747044917262e-05\n",
      "global step 4430, epoch: 2, batch: 2080/2350, loss: 5.95628,  speed: 0.66 step/s, lr: 4.5085106382978726e-05\n",
      "global step 4440, epoch: 2, batch: 2090/2350, loss: 5.97854,  speed: 0.66 step/s, lr: 4.5061465721040191e-05\n",
      "global step 4450, epoch: 2, batch: 2100/2350, loss: 5.94962,  speed: 0.66 step/s, lr: 4.5037825059101656e-05\n",
      "global step 4460, epoch: 2, batch: 2110/2350, loss: 5.95940,  speed: 0.66 step/s, lr: 4.5014184397163120e-05\n",
      "global step 4470, epoch: 2, batch: 2120/2350, loss: 5.95766,  speed: 0.66 step/s, lr: 4.4990543735224591e-05\n",
      "global step 4480, epoch: 2, batch: 2130/2350, loss: 5.96042,  speed: 0.66 step/s, lr: 4.4966903073286056e-05\n",
      "global step 4490, epoch: 2, batch: 2140/2350, loss: 5.97002,  speed: 0.66 step/s, lr: 4.4943262411347520e-05\n",
      "global step 4500, epoch: 2, batch: 2150/2350, loss: 5.99224,  speed: 0.66 step/s, lr: 4.4919621749408985e-05\n",
      "Processing example: 1000\n",
      "time per 1000: 24.017693281173706\n",
      "global step: 4500, eval dev Exact Mactch: 0.00000, f1_score: 0.10304\n",
      "global step 4510, epoch: 2, batch: 2160/2350, loss: 5.94461,  speed: 0.16 step/s, lr: 4.4895981087470450e-05\n",
      "global step 4520, epoch: 2, batch: 2170/2350, loss: 5.97128,  speed: 0.66 step/s, lr: 4.4872340425531914e-05\n",
      "global step 4530, epoch: 2, batch: 2180/2350, loss: 5.96508,  speed: 0.66 step/s, lr: 4.4848699763593385e-05\n",
      "global step 4540, epoch: 2, batch: 2190/2350, loss: 5.98032,  speed: 0.66 step/s, lr: 4.4825059101654850e-05\n",
      "global step 4550, epoch: 2, batch: 2200/2350, loss: 5.95241,  speed: 0.66 step/s, lr: 4.4801418439716314e-05\n",
      "global step 4560, epoch: 2, batch: 2210/2350, loss: 5.96696,  speed: 0.65 step/s, lr: 4.4777777777777779e-05\n",
      "global step 4570, epoch: 2, batch: 2220/2350, loss: 5.92490,  speed: 0.66 step/s, lr: 4.4754137115839244e-05\n",
      "global step 4580, epoch: 2, batch: 2230/2350, loss: 5.95564,  speed: 0.66 step/s, lr: 4.4730496453900708e-05\n",
      "global step 4590, epoch: 2, batch: 2240/2350, loss: 5.96037,  speed: 0.66 step/s, lr: 4.4706855791962179e-05\n",
      "global step 4600, epoch: 2, batch: 2250/2350, loss: 5.97882,  speed: 0.66 step/s, lr: 4.4683215130023644e-05\n",
      "global step 4610, epoch: 2, batch: 2260/2350, loss: 5.94738,  speed: 0.66 step/s, lr: 4.4659574468085108e-05\n",
      "global step 4620, epoch: 2, batch: 2270/2350, loss: 5.96410,  speed: 0.66 step/s, lr: 4.4635933806146573e-05\n",
      "global step 4630, epoch: 2, batch: 2280/2350, loss: 5.96521,  speed: 0.66 step/s, lr: 4.4612293144208038e-05\n",
      "global step 4640, epoch: 2, batch: 2290/2350, loss: 5.93815,  speed: 0.66 step/s, lr: 4.4588652482269502e-05\n",
      "global step 4650, epoch: 2, batch: 2300/2350, loss: 5.95283,  speed: 0.66 step/s, lr: 4.4565011820330973e-05\n",
      "global step 4660, epoch: 2, batch: 2310/2350, loss: 5.94067,  speed: 0.66 step/s, lr: 4.4541371158392438e-05\n",
      "global step 4670, epoch: 2, batch: 2320/2350, loss: 5.98736,  speed: 0.66 step/s, lr: 4.4517730496453902e-05\n",
      "global step 4680, epoch: 2, batch: 2330/2350, loss: 5.97886,  speed: 0.66 step/s, lr: 4.4494089834515367e-05\n",
      "global step 4690, epoch: 2, batch: 2340/2350, loss: 5.95458,  speed: 0.66 step/s, lr: 4.4470449172576832e-05\n",
      "global step 4700, epoch: 2, batch: 2350/2350, loss: 5.90042,  speed: 0.68 step/s, lr: 4.4446808510638296e-05\n",
      "global step 4710, epoch: 3, batch: 10/2350, loss: 5.94002,  speed: 0.69 step/s, lr: 4.4423167848699767e-05\n",
      "global step 4720, epoch: 3, batch: 20/2350, loss: 5.96778,  speed: 0.66 step/s, lr: 4.4399527186761232e-05\n",
      "global step 4730, epoch: 3, batch: 30/2350, loss: 5.98718,  speed: 0.66 step/s, lr: 4.4375886524822696e-05\n",
      "global step 4740, epoch: 3, batch: 40/2350, loss: 5.96573,  speed: 0.66 step/s, lr: 4.4352245862884161e-05\n",
      "global step 4750, epoch: 3, batch: 50/2350, loss: 5.95229,  speed: 0.66 step/s, lr: 4.4328605200945626e-05\n",
      "global step 4760, epoch: 3, batch: 60/2350, loss: 5.96031,  speed: 0.66 step/s, lr: 4.4304964539007090e-05\n",
      "global step 4770, epoch: 3, batch: 70/2350, loss: 5.96210,  speed: 0.66 step/s, lr: 4.4281323877068561e-05\n",
      "global step 4780, epoch: 3, batch: 80/2350, loss: 5.97658,  speed: 0.66 step/s, lr: 4.4257683215130026e-05\n",
      "global step 4790, epoch: 3, batch: 90/2350, loss: 5.93007,  speed: 0.66 step/s, lr: 4.4234042553191490e-05\n",
      "global step 4800, epoch: 3, batch: 100/2350, loss: 5.94316,  speed: 0.66 step/s, lr: 4.4210401891252955e-05\n",
      "global step 4810, epoch: 3, batch: 110/2350, loss: 5.94569,  speed: 0.66 step/s, lr: 4.4186761229314420e-05\n",
      "global step 4820, epoch: 3, batch: 120/2350, loss: 5.96765,  speed: 0.66 step/s, lr: 4.4163120567375884e-05\n",
      "global step 4830, epoch: 3, batch: 130/2350, loss: 6.00594,  speed: 0.66 step/s, lr: 4.4139479905437355e-05\n",
      "global step 4840, epoch: 3, batch: 140/2350, loss: 5.96275,  speed: 0.66 step/s, lr: 4.4115839243498820e-05\n",
      "global step 4850, epoch: 3, batch: 150/2350, loss: 5.96246,  speed: 0.66 step/s, lr: 4.4092198581560284e-05\n",
      "global step 4860, epoch: 3, batch: 160/2350, loss: 5.94758,  speed: 0.66 step/s, lr: 4.4068557919621749e-05\n",
      "global step 4870, epoch: 3, batch: 170/2350, loss: 5.95803,  speed: 0.66 step/s, lr: 4.4044917257683214e-05\n",
      "global step 4880, epoch: 3, batch: 180/2350, loss: 5.95041,  speed: 0.66 step/s, lr: 4.4021276595744685e-05\n",
      "global step 4890, epoch: 3, batch: 190/2350, loss: 5.94439,  speed: 0.66 step/s, lr: 4.3997635933806149e-05\n",
      "global step 4900, epoch: 3, batch: 200/2350, loss: 5.95577,  speed: 0.66 step/s, lr: 4.3973995271867614e-05\n",
      "global step 4910, epoch: 3, batch: 210/2350, loss: 5.94356,  speed: 0.66 step/s, lr: 4.3950354609929078e-05\n",
      "global step 4920, epoch: 3, batch: 220/2350, loss: 5.94629,  speed: 0.66 step/s, lr: 4.3926713947990543e-05\n",
      "global step 4930, epoch: 3, batch: 230/2350, loss: 5.94652,  speed: 0.66 step/s, lr: 4.3903073286052008e-05\n",
      "global step 4940, epoch: 3, batch: 240/2350, loss: 5.96572,  speed: 0.66 step/s, lr: 4.3879432624113479e-05\n",
      "global step 4950, epoch: 3, batch: 250/2350, loss: 5.93845,  speed: 0.66 step/s, lr: 4.3855791962174943e-05\n",
      "global step 4960, epoch: 3, batch: 260/2350, loss: 5.97482,  speed: 0.66 step/s, lr: 4.3832151300236408e-05\n",
      "global step 4970, epoch: 3, batch: 270/2350, loss: 5.94784,  speed: 0.66 step/s, lr: 4.3808510638297872e-05\n",
      "global step 4980, epoch: 3, batch: 280/2350, loss: 5.93090,  speed: 0.66 step/s, lr: 4.3784869976359337e-05\n",
      "global step 4990, epoch: 3, batch: 290/2350, loss: 5.96539,  speed: 0.66 step/s, lr: 4.3761229314420802e-05\n",
      "global step 5000, epoch: 3, batch: 300/2350, loss: 5.96900,  speed: 0.66 step/s, lr: 4.3737588652482273e-05\n",
      "Processing example: 1000\n",
      "time per 1000: 24.028059244155884\n",
      "global step: 5000, eval dev Exact Mactch: 0.00141, f1_score: 0.08708\n",
      "global step 5010, epoch: 3, batch: 310/2350, loss: 5.94759,  speed: 0.16 step/s, lr: 4.3713947990543737e-05\n",
      "global step 5020, epoch: 3, batch: 320/2350, loss: 5.94598,  speed: 0.66 step/s, lr: 4.3690307328605202e-05\n",
      "global step 5030, epoch: 3, batch: 330/2350, loss: 5.93380,  speed: 0.66 step/s, lr: 4.3666666666666666e-05\n",
      "global step 5040, epoch: 3, batch: 340/2350, loss: 5.96838,  speed: 0.66 step/s, lr: 4.3643026004728131e-05\n",
      "global step 5050, epoch: 3, batch: 350/2350, loss: 5.95199,  speed: 0.66 step/s, lr: 4.3619385342789596e-05\n",
      "global step 5060, epoch: 3, batch: 360/2350, loss: 5.96741,  speed: 0.66 step/s, lr: 4.3595744680851067e-05\n",
      "global step 5070, epoch: 3, batch: 370/2350, loss: 5.95854,  speed: 0.66 step/s, lr: 4.3572104018912531e-05\n",
      "global step 5080, epoch: 3, batch: 380/2350, loss: 5.95704,  speed: 0.66 step/s, lr: 4.3548463356973996e-05\n",
      "global step 5090, epoch: 3, batch: 390/2350, loss: 5.95222,  speed: 0.66 step/s, lr: 4.3524822695035460e-05\n",
      "global step 5100, epoch: 3, batch: 400/2350, loss: 5.94022,  speed: 0.66 step/s, lr: 4.3501182033096925e-05\n",
      "global step 5110, epoch: 3, batch: 410/2350, loss: 5.94955,  speed: 0.66 step/s, lr: 4.3477541371158390e-05\n",
      "global step 5120, epoch: 3, batch: 420/2350, loss: 5.96744,  speed: 0.66 step/s, lr: 4.3453900709219861e-05\n",
      "global step 5130, epoch: 3, batch: 430/2350, loss: 5.95133,  speed: 0.66 step/s, lr: 4.3430260047281325e-05\n",
      "global step 5140, epoch: 3, batch: 440/2350, loss: 5.95922,  speed: 0.66 step/s, lr: 4.3406619385342790e-05\n",
      "global step 5150, epoch: 3, batch: 450/2350, loss: 5.95419,  speed: 0.66 step/s, lr: 4.3382978723404254e-05\n",
      "global step 5160, epoch: 3, batch: 460/2350, loss: 5.96360,  speed: 0.66 step/s, lr: 4.3359338061465719e-05\n",
      "global step 5170, epoch: 3, batch: 470/2350, loss: 5.94900,  speed: 0.66 step/s, lr: 4.3335697399527184e-05\n",
      "global step 5180, epoch: 3, batch: 480/2350, loss: 5.97259,  speed: 0.66 step/s, lr: 4.3312056737588655e-05\n",
      "global step 5190, epoch: 3, batch: 490/2350, loss: 5.95559,  speed: 0.66 step/s, lr: 4.3288416075650119e-05\n",
      "global step 5200, epoch: 3, batch: 500/2350, loss: 5.93652,  speed: 0.66 step/s, lr: 4.3264775413711584e-05\n",
      "global step 5210, epoch: 3, batch: 510/2350, loss: 5.96012,  speed: 0.66 step/s, lr: 4.3241134751773048e-05\n",
      "global step 5220, epoch: 3, batch: 520/2350, loss: 5.96750,  speed: 0.66 step/s, lr: 4.3217494089834513e-05\n",
      "global step 5230, epoch: 3, batch: 530/2350, loss: 5.97238,  speed: 0.66 step/s, lr: 4.3193853427895978e-05\n",
      "global step 5240, epoch: 3, batch: 540/2350, loss: 5.95997,  speed: 0.66 step/s, lr: 4.3170212765957449e-05\n",
      "global step 5250, epoch: 3, batch: 550/2350, loss: 5.96856,  speed: 0.66 step/s, lr: 4.3146572104018913e-05\n",
      "global step 5260, epoch: 3, batch: 560/2350, loss: 5.93391,  speed: 0.66 step/s, lr: 4.3122931442080378e-05\n",
      "global step 5270, epoch: 3, batch: 570/2350, loss: 5.95938,  speed: 0.66 step/s, lr: 4.3099290780141842e-05\n",
      "global step 5280, epoch: 3, batch: 580/2350, loss: 5.96803,  speed: 0.66 step/s, lr: 4.3075650118203314e-05\n",
      "global step 5290, epoch: 3, batch: 590/2350, loss: 5.97364,  speed: 0.66 step/s, lr: 4.3052009456264778e-05\n",
      "global step 5300, epoch: 3, batch: 600/2350, loss: 5.93827,  speed: 0.66 step/s, lr: 4.3028368794326243e-05\n",
      "global step 5310, epoch: 3, batch: 610/2350, loss: 5.96140,  speed: 0.66 step/s, lr: 4.3004728132387714e-05\n",
      "global step 5320, epoch: 3, batch: 620/2350, loss: 5.96303,  speed: 0.66 step/s, lr: 4.2981087470449179e-05\n",
      "global step 5330, epoch: 3, batch: 630/2350, loss: 5.96797,  speed: 0.66 step/s, lr: 4.2957446808510643e-05\n",
      "global step 5340, epoch: 3, batch: 640/2350, loss: 5.93177,  speed: 0.66 step/s, lr: 4.2933806146572108e-05\n",
      "global step 5350, epoch: 3, batch: 650/2350, loss: 5.96529,  speed: 0.66 step/s, lr: 4.2910165484633572e-05\n",
      "global step 5360, epoch: 3, batch: 660/2350, loss: 5.96188,  speed: 0.66 step/s, lr: 4.2886524822695037e-05\n",
      "global step 5370, epoch: 3, batch: 670/2350, loss: 5.95577,  speed: 0.66 step/s, lr: 4.2862884160756508e-05\n",
      "global step 5380, epoch: 3, batch: 680/2350, loss: 5.94377,  speed: 0.66 step/s, lr: 4.2839243498817973e-05\n",
      "global step 5390, epoch: 3, batch: 690/2350, loss: 5.96577,  speed: 0.66 step/s, lr: 4.2815602836879437e-05\n",
      "global step 5400, epoch: 3, batch: 700/2350, loss: 5.93306,  speed: 0.66 step/s, lr: 4.2791962174940902e-05\n",
      "global step 5410, epoch: 3, batch: 710/2350, loss: 5.98325,  speed: 0.66 step/s, lr: 4.2768321513002366e-05\n",
      "global step 5420, epoch: 3, batch: 720/2350, loss: 5.94460,  speed: 0.66 step/s, lr: 4.2744680851063831e-05\n",
      "global step 5430, epoch: 3, batch: 730/2350, loss: 5.94512,  speed: 0.66 step/s, lr: 4.2721040189125302e-05\n",
      "global step 5440, epoch: 3, batch: 740/2350, loss: 5.94013,  speed: 0.66 step/s, lr: 4.2697399527186767e-05\n",
      "global step 5450, epoch: 3, batch: 750/2350, loss: 5.95443,  speed: 0.66 step/s, lr: 4.2673758865248231e-05\n",
      "global step 5460, epoch: 3, batch: 760/2350, loss: 5.94018,  speed: 0.66 step/s, lr: 4.2650118203309696e-05\n",
      "global step 5470, epoch: 3, batch: 770/2350, loss: 5.95809,  speed: 0.66 step/s, lr: 4.2626477541371160e-05\n",
      "global step 5480, epoch: 3, batch: 780/2350, loss: 5.94267,  speed: 0.66 step/s, lr: 4.2602836879432625e-05\n",
      "global step 5490, epoch: 3, batch: 790/2350, loss: 5.94947,  speed: 0.66 step/s, lr: 4.2579196217494096e-05\n",
      "global step 5500, epoch: 3, batch: 800/2350, loss: 5.93813,  speed: 0.66 step/s, lr: 4.2555555555555561e-05\n",
      "Processing example: 1000\n",
      "time per 1000: 24.03506875038147\n",
      "global step: 5500, eval dev Exact Mactch: 0.00000, f1_score: 0.04841\n",
      "global step 5510, epoch: 3, batch: 810/2350, loss: 5.95565,  speed: 0.16 step/s, lr: 4.2531914893617025e-05\n",
      "global step 5520, epoch: 3, batch: 820/2350, loss: 5.94138,  speed: 0.66 step/s, lr: 4.2508274231678490e-05\n",
      "global step 5530, epoch: 3, batch: 830/2350, loss: 5.95583,  speed: 0.66 step/s, lr: 4.2484633569739954e-05\n",
      "global step 5540, epoch: 3, batch: 840/2350, loss: 5.96105,  speed: 0.66 step/s, lr: 4.2460992907801426e-05\n",
      "global step 5550, epoch: 3, batch: 850/2350, loss: 5.92618,  speed: 0.66 step/s, lr: 4.2437352245862890e-05\n",
      "global step 5560, epoch: 3, batch: 860/2350, loss: 5.96687,  speed: 0.66 step/s, lr: 4.2413711583924355e-05\n",
      "global step 5570, epoch: 3, batch: 870/2350, loss: 5.96385,  speed: 0.66 step/s, lr: 4.2390070921985819e-05\n",
      "global step 5580, epoch: 3, batch: 880/2350, loss: 5.95933,  speed: 0.66 step/s, lr: 4.2366430260047284e-05\n",
      "global step 5590, epoch: 3, batch: 890/2350, loss: 5.94913,  speed: 0.66 step/s, lr: 4.2342789598108748e-05\n",
      "global step 5600, epoch: 3, batch: 900/2350, loss: 5.94627,  speed: 0.66 step/s, lr: 4.2319148936170220e-05\n",
      "global step 5610, epoch: 3, batch: 910/2350, loss: 5.94821,  speed: 0.66 step/s, lr: 4.2295508274231684e-05\n",
      "global step 5620, epoch: 3, batch: 920/2350, loss: 5.93894,  speed: 0.66 step/s, lr: 4.2271867612293149e-05\n",
      "global step 5630, epoch: 3, batch: 930/2350, loss: 5.97171,  speed: 0.66 step/s, lr: 4.2248226950354613e-05\n",
      "global step 5640, epoch: 3, batch: 940/2350, loss: 5.97337,  speed: 0.66 step/s, lr: 4.2224586288416078e-05\n",
      "global step 5650, epoch: 3, batch: 950/2350, loss: 5.94734,  speed: 0.66 step/s, lr: 4.2200945626477542e-05\n",
      "global step 5660, epoch: 3, batch: 960/2350, loss: 5.97869,  speed: 0.66 step/s, lr: 4.2177304964539014e-05\n",
      "global step 5670, epoch: 3, batch: 970/2350, loss: 5.96735,  speed: 0.66 step/s, lr: 4.2153664302600478e-05\n",
      "global step 5680, epoch: 3, batch: 980/2350, loss: 5.94722,  speed: 0.66 step/s, lr: 4.2130023640661943e-05\n",
      "global step 5690, epoch: 3, batch: 990/2350, loss: 5.94881,  speed: 0.66 step/s, lr: 4.2106382978723407e-05\n",
      "global step 5700, epoch: 3, batch: 1000/2350, loss: 5.96416,  speed: 0.66 step/s, lr: 4.2082742316784872e-05\n",
      "global step 5710, epoch: 3, batch: 1010/2350, loss: 5.96845,  speed: 0.66 step/s, lr: 4.2059101654846336e-05\n",
      "global step 5720, epoch: 3, batch: 1020/2350, loss: 5.94110,  speed: 0.66 step/s, lr: 4.2035460992907808e-05\n",
      "global step 5730, epoch: 3, batch: 1030/2350, loss: 5.94899,  speed: 0.66 step/s, lr: 4.2011820330969272e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e7f959365c7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdo_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-858764b1c692>\u001b[0m in \u001b[0;36mdo_train\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_built\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlenlp/transformers/roberta/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/nn.py\u001b[0m in \u001b[0;36munstack\u001b[0;34m(x, axis, num)\u001b[0m\n\u001b[1;32m  10233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10234\u001b[0m             \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'axis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10237\u001b[0m     \u001b[0mhelper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayerHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unstack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "do_train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_predict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20038/20038 [00:00<00:00, 58154.62it/s]\n",
      "100%|██████████| 7236/7236 [00:00<00:00, 25550.67it/s]\n",
      "100%|██████████| 3222/3222 [00:00<00:00, 28489.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.datasets import load_dataset\r\n",
    "\r\n",
    "\r\n",
    "train_robust, dev_robust = load_dataset('dureader_robust', splits=('train', 'dev'))\r\n",
    "train_cmrc, dev_cmrc = load_dataset('cmrc2018', splits=['train', 'dev'])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14520, 1417, 10142, 3219)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_robust), len(dev_robust), len(train_cmrc), len(dev_cmrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx, example in enumerate(train_robust):\r\n",
    "    train_dataset.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx, example in enumerate(train_cmrc):\r\n",
    "    train_dataset.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24662, 24662)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(train_robust) + len(train_cmrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_robust.label_list), print(train_cmrc.label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx, example in enumerate(dev_cmrc):\r\n",
    "    train_dataset.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27881"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import MapDataset\r\n",
    "\r\n",
    "train_ds = MapDataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paddlenlp.datasets.dataset.MapDataset"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20038/20038 [00:00<00:00, 64438.60it/s]\n"
     ]
    }
   ],
   "source": [
    "train_robust, dev_ds = load_dataset('dureader_robust', splits=['train', 'dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_robust = list(train_robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 14520)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_robust), len(train_robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0a25cb4bc1ab6f474c699884e04601e4',\n",
       " 'title': '',\n",
       " 'context': '第35集雪见缓缓张开眼睛，景天又惊又喜之际，长卿和紫萱的仙船驶至，见众人无恙，也十分高兴。众人登船，用尽合力把自身的真气和水分输给她。雪见终于醒过来了，但却一脸木然，全无反应。众人向常胤求助，却发现人世界竟没有雪见的身世纪录。长卿询问清微的身世，清微语带双关说一切上了天界便有答案。长卿驾驶仙船，众人决定立马动身，往天界而去。众人来到一荒山，长卿指出，魔界和天界相连。由魔界进入通过神魔之井，便可登天。众人至魔界入口，仿若一黑色的蝙蝠洞，但始终无法进入。后来花楹发现只要有翅膀便能飞入。于是景天等人打下许多乌鸦，模仿重楼的翅膀，制作数对翅膀状巨物。刚佩戴在身，便被吸入洞口。众人摔落在地，抬头发现魔界守卫。景天和众魔套交情，自称和魔尊重楼相熟，众魔不理，打了起来。',\n",
       " 'question': '仙剑奇侠传3第几集上天界',\n",
       " 'answers': ['第35集'],\n",
       " 'answer_starts': [0]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_robust[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_robust_aug = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import ErnieGramTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-07-18 23:13:02,309] [    INFO] - Downloading vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/ernie_gram_zh/vocab.txt\n",
      "100%|██████████| 78/78 [00:00<00:00, 3670.26it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ErnieGramTokenizer.from_pretrained('ernie-gram-zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for example in train_robust:\r\n",
    "    train_robust_aug.append(example)\r\n",
    "    for it in range(5):\r\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
